<!DOCTYPE html>
<html>
<head>
  <title>Machine Learning with R - II</title>
  <meta charset="utf-8">
  <meta name="description" content="Machine Learning with R - II">
  <meta name="author" content="Ilan Man">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
    
</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
    <!-- END LOGO SLIDE -->
    

    <!-- TITLE SLIDE -->
    <!-- Should I move this to a Local Layout File? -->
    <slide class="title-slide segue nobackground">
      <hgroup class="auto-fadein">
        <h1>Machine Learning with R - II</h1>
        <h2></h2>
        <p>Ilan Man<br/>Strategy Operations  @ Squarespace</p>
      </hgroup>
          </slide>

    <!-- SLIDES -->
      <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Agenda</h2>
  </hgroup>
  <article>
    <p><space></p>

<ol>
<li>Logistic Regression</li>
<li>Principle Component Analysis</li>
<li>Clustering</li>
<li>Trees</li>
<li>Missing data</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Objectives</h2>
  </hgroup>
  <article>
    <p><space></p>

<ol>
<li>Understand some popular algorithms and techniques</li>
<li>Learn how to tune parameters</li>
<li>Practice R</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p>x &lt;- 1:10
log_ex &lt;- data.frame(Y=c(rnorm(5,0,0.01),rnorm(5,5,0.01)),X=x)
ggplot(log_ex,aes(X,Y)) + geom_point(color=&#39;blue&#39;,size=3) + stat_smooth(method=&#39;lm&#39;,se=F,color=&#39;green&#39;,size=1)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<pre><code class="r">library(&quot;MASS&quot;)
library(ggplot2)
data(menarche)
log_data &lt;- data.frame(Y=menarche$Menarche/menarche$Total)
log_data$X &lt;- menarche$Age

glm.out &lt;- glm(cbind(Menarche, Total-Menarche) ~ Age,family=binomial(logit), data=menarche)
lm.out &lt;- lm(Y~X, data=log_data)

log_data$fitted &lt;- glm.out$fitted

data_points &lt;- ggplot(log_data) + geom_point(aes(x=X,y=Y),color=&#39;blue&#39;,size=3)
line_points &lt;- data_points + geom_abline(intercept = coef(lm.out)[1], slope = coef(lm.out)[2],color=&#39;green&#39;,size=1)
curve_points &lt;- line_points + geom_line(aes(x=X,y=fitted),color=&#39;red&#39;,size=1) 
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Notation</h1>

<p><space></p>

<ul>
<li>type of regression to predict the probability of being in a class

<ul>
<li>typical to set threshold to 0.5</li>
</ul></li>
<li>assumes error terms are Binomially distributed

<ul>
<li>which generates 1&#39;s and 0&#39;s as the error term</li>
</ul></li>
<li>sigmoid or logistic function: \(g(z) = \frac{1}{1+e^{-z}}\)

<ul>
<li>interpret the output as \(P(Y=1 | X)\)</li>
<li>bounded by 0 and 1</li>
</ul></li>
</ul>

<pre><code class="r">curve(1/(1+exp(-x)), from = -10, to = 10, ylab=&quot;P(Y=1|X)&quot;, col = &#39;red&#39;, lwd = 3.0)
abline(a=0.5, b=0, lty=2, col=&#39;blue&#39;, lwd = 3.0)
</code></pre>

<p><img src="figure/unnamed-chunk-2.png" alt="plot of chunk unnamed-chunk-2"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>The hypothesis function, \(h_{\theta}(x)\), is P(Y=1|X)</li>
<li>Linear Regression --&gt; \(h_{\theta}(x) = \theta x^{T}\)</li>
<li>Logistic Regression --&gt; \(h_{\theta}(x) = g(\theta x^{T})\) 
<br>
where \(g(z) = \frac{1}{1+e^{-z}}\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>Re-arranging \(Y = \frac{1}{1+e^{-\theta x^{T}}}\) yields
<br>
<br>
\(\log{\frac{Y}{1 - Y}} = \theta x^{T}\)<br></li>
<li>log odds are linear in X</li>
<li>this is called the logit of theta - this is linear in X</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>So \(h_{\theta}(x) = \frac{1}{1+e^{-\theta x^{T}}}\)</li>
<li>What is the cost function?</li>
<li>Why can&#39;t we use the same cost function as before?

<ul>
<li>logistic residuals are Binomially distributed - not NORMAL</li>
<li>the regression function is not linear in X</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>Define cost function as:</li>
</ul>

<p>\(cost(h_{\theta}(x)):\)<br>
\(= -\log(x),   y = 1\)<br>
\(= -\log(1-x),   y = 0\)</p>

<p><img src="figure/cost_curves1.png" alt="plot of chunk cost_curves"> <img src="figure/cost_curves2.png" alt="plot of chunk cost_curves"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>using statistics, it can be shown that<br>
\(cost(h_{\theta}(x), y) = -y \log(h_{\theta}(x)) + (1-y) \log(1-h_{\theta}(x))\)<br></li>
<li>Logistic regression cost function is then<br>
\(cost(h_{\theta}(x), y)  = \frac{1}{m} \sum_{i=1}^{m} -y \log(h_{\theta}(x)) + (1-y) \log(1-h_{\theta}(x))\)</li>
<li>Minimize the cost</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<p><img src="figure/grad_ex_plot.png" alt="plot of chunk grad_ex_plot"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code class="r">x &lt;- cbind(1,x)  #Add ones to x  
theta&lt;- c(0,0)  # initalize theta vector 
m &lt;- nrow(x)  # Number of the observations 
grad_cost &lt;- function(X,y,theta) return(sum(((X%*%theta)- y)^2))
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code class="r">gradDescent&lt;-function(X,y,theta,iterations,alpha){
  m &lt;- length(y)
  grad &lt;- rep(0,length(theta))
  cost.df &lt;- data.frame(cost=0,theta=0)

  for (i in 1:iterations){
    h &lt;- X%*%theta
    grad &lt;-  (t(X)%*%(h - y))/m
    theta &lt;- theta - alpha * grad
    cost.df &lt;- rbind(cost.df,c(grad_cost(X,y,theta),theta))    
  }  

  return(list(theta,cost.df))
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code class="r">## initialize X, y and theta
X1&lt;-matrix(ncol=1,nrow=nrow(df),cbind(1,df$X))
Y1&lt;-matrix(ncol=1,nrow=nrow(df),df$Y)

init_theta&lt;-as.matrix(c(0))
grad_cost(X1,Y1,init_theta)
</code></pre>

<pre><code>[1] 5296
</code></pre>

<pre><code class="r">iterations = 100
alpha = 0.1
results &lt;- gradDescent(X1,Y1,init_theta,iterations,alpha)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code>## Error: object &#39;cost.df&#39; not found
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code class="r">grad_cost(X1,Y1,theta[[1]])
</code></pre>

<pre><code>[1] 331.2
</code></pre>

<pre><code class="r">## Make some predictions
intercept &lt;- df[df$X==0,]$Y
pred &lt;- function (x) return(intercept+c(x)%*%theta)
new_points &lt;- c(0.1,0.5,0.8,1.1)
new_preds &lt;- data.frame(X=new_points,Y=sapply(new_points,pred))
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code class="r">ggplot(data=df,aes(x=X,y=Y))+geom_point(size=2)
</code></pre>

<p><img src="figure/new_point1.png" alt="plot of chunk new_point"> </p>

<pre><code class="r">ggplot(data=df,aes(x=X,y=Y))+geom_point()+geom_point(data=new_preds,aes(x=X,y=Y,color=&#39;red&#39;),size=3)+scale_colour_discrete(guide = FALSE)
</code></pre>

<p><img src="figure/new_point2.png" alt="plot of chunk new_point"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent - summary</h1>

<p><space></p>

<ul>
<li>minimization algorithm</li>
<li>approximation, non-closed form solution</li>
<li>good for large number of examples</li>
<li>hard to select the right \(\alpha\)</li>
<li>traditional looping is slow - optimization algorithms are used in practice</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Summary</h1>

<p><space></p>

<ul>
<li>very popular classification algorithm</li>
</ul>

<h2>- based on Binomial error terms, i.e. 1&#39;s and 0&#39;s</h2>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>used widely in modern data analysis</li>
<li>not well understood</li>
<li>intuition: reduce data into only relevant dimensions</li>
<li>the goal of PCA is to compute the most meaningful was to re-express noisy data, revealing the hidden structure</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>first big assumption: linearity</li>
<li>\(PX=Y\)

<ul>
<li>\(X\) is original dataset, \(P\) is a transformation of \(X\) into \(Y\)</li>
</ul></li>
<li>how do we choose \(P\)?

<ul>
<li>reduce noise</li>
<li>maximize variance</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li><p>covariance matrix</p>

<ul>
<li>\(C = X*X^{T}\)</li>
</ul></li>
<li><p>restated goals are</p>

<ul>
<li>minimize covariance and maximize variance</li>
<li>the optimizal \(C\) is a diagonal matrix, off diagonals are = 0</li>
</ul></li>
</ul>

<hr>

<h2>Principle Component Analysis</h2>

<h1>Concepts</h1>

<p><space></p>

<ul>
<li>summary of assumptions

<ul>
<li>linearity (non-linear is a kernel PCA)</li>
<li>largest variance indicates most signal, low variance = noise</li>
<li>orthogonal components - makes the linear algebra easier</li>
<li>assumes data is normally distributed, otherwise PCA might not diagonalize matrix</li>
<li>can use ICA</li>
<li>but most data is normal and PCA is robust to slight deviance from normality</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-23" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<ul>
<li>\(Ax = \lambdax\)

<ul>
<li>\(\lambda\) is an eigenvalue of \(A\) and \(x\) is an eigenvector of \(A\)</li>
</ul></li>
<li>\(Ax - \lambdaIx = 0\)</li>
<li>\((A - \lambdaI)x = 0\)</li>
<li>\(\det(A - \lambdaI)\) = 0</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-24" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<p>\(\[A=\left[{\begin{array}{cc}5 & 2 \\2 & 5\\\end{array}\right ]\]\)</p>

<p>A = matrix(c(5,2,2,5),nrow=2)
I = diag(nrow(A))
|A - L<em>I| = 0
det(c(5-l,2,2,5-l))
(5-l)</em>(5-l) - 4 = 0
25 - 10l + l<sup>2</sup> - 4 = 0
l<sup>2</sup> - 10l + 21 = 0
roots &lt;- Re(polyroot(c(21,-10,1)))</p>

<pre><code></code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-25" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<ul>
<li>when lambda = -3
Ax = 3x
5x1 + 2x2 = 3x1
2x1 + 5x2 = 3x2
x1=-x2</li>
<li>one eigenvector = [1 -1]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-26" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<ul>
<li>when lambda = 7
5x1 + 2x2 = 7x1
2x2 + 5x2 = 7x2
x1 = x2</li>
<li>another eigenvector = [1 1]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-27" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<p>A%<em>%c(1,-1) == 3 * as.matrix(c(1,-1))
A%</em>%c(1,1) == 7 * as.matrix(c(1,1))
roots</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-28" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<ul>
<li>check
m &lt;- matrix(c(1,-1,1,1),ncol=2)
m &lt;- m/sqrt(norm(m))
as.matrix(m%<em>%diag(roots)%</em>%t(m))</li>
<li>lambda is a diagonal matrix, with 0 off diagonals</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-29" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p>PX = Y</p>

<p>CY = (1/(n-1))*YYt
=PX(PX)t
=PXXtPt
=PAPt</p>

<h1>P is a matrix with columns that are eigenvectors</h1>

<h1>A is a diagonalized matrix of eigenvalues (by linear algebra) and symmetric</h1>

<p>A = EDEt</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-30" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<h1>each row of P should be an eigenvector of A</h1>

<p>P=Et</p>

<h1>also note that Pt = P-1 (linear algebra)</h1>

<p>A = PtDP
CY = PPtDPPt
= (1/(n-1))*D</p>

<h1>D is a diagonal matrix, depending on how we choose P</h1>

<h1>therefore CY is diagonalized</h1>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-31" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">data &lt;- read.csv(&#39;tennis_data_2013.csv&#39;)
</code></pre>

<pre><code>## Warning: cannot open file &#39;tennis_data_2013.csv&#39;: No such file or
## directory
</code></pre>

<pre><code>## Error: cannot open the connection
</code></pre>

<pre><code class="r">data$Player1 &lt;- as.character(data$Player1)
</code></pre>

<pre><code>## Error: replacement has 0 rows, data has 6497
</code></pre>

<pre><code class="r">data$Player2 &lt;- as.character(data$Player2)
</code></pre>

<pre><code>## Error: replacement has 0 rows, data has 6497
</code></pre>

<pre><code class="r">tennis &lt;- data
m &lt;- length(data)

for (i in 10:m){
  tennis[,i] &lt;- ifelse(is.na(data[,i]),0,data[,i])
}

str(tennis)
</code></pre>

<pre><code>## &#39;data.frame&#39;:    6497 obs. of  13 variables:
##  $ type                : Factor w/ 2 levels &quot;red&quot;,&quot;white&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ fixed.acidity       : num  7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 7.5 ...
##  $ volatile.acidity    : num  0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.5 ...
##  $ citric.acid         : num  0 0 0.04 0.56 0 0 0.06 0 0.02 0.36 ...
##  $ residual.sugar      : num  1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 6.1 ...
##  $ chlorides           : num  0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.071 ...
##  $ free.sulfur.dioxide : num  11 25 15 17 11 13 15 15 9 17 ...
##  $ total.sulfur.dioxide: num  34 67 54 60 34 40 59 21 18 102 ...
##  $ density             : num  0.998 0.997 0.997 0.998 0.998 ...
##  $ pH                  : num  3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ...
##  $ sulphates           : num  0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ...
##  $ alcohol             : num  9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ...
##  $ quality             : int  5 5 5 6 5 5 5 7 7 5 ...
</code></pre>

<pre><code class="r">features &lt;- tennis[,10:m]

head(features)
</code></pre>

<pre><code>##     pH sulphates alcohol quality
## 1 3.51      0.56     9.4       5
## 2 3.20      0.68     9.8       5
## 3 3.26      0.65     9.8       5
## 4 3.16      0.58     9.8       6
## 5 3.51      0.56     9.4       5
## 6 3.51      0.56     9.4       5
</code></pre>

<pre><code class="r">str(features)
</code></pre>

<pre><code>## &#39;data.frame&#39;:    6497 obs. of  4 variables:
##  $ pH       : num  3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ...
##  $ sulphates: num  0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ...
##  $ alcohol  : num  9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ...
##  $ quality  : int  5 5 5 6 5 5 5 7 7 5 ...
</code></pre>

<pre><code class="r">dim(features)
</code></pre>

<pre><code>## [1] 6497    4
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-32" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">scaled_features &lt;- as.matrix(scale(features))
Cx &lt;- cov(scaled_features)
eigenvalues &lt;- eigen(Cx)$values
eigenvectors &lt;- eigen(Cx)$vectors
PC &lt;- scaled_features %*% eigenvectors
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-33" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">Cy &lt;- cov(PC)
sum(round(diag(Cy) - eigenvalues,5))
</code></pre>

<pre><code>## [1] 0
</code></pre>

<pre><code class="r">sum(round(Cy[upper.tri(Cy)],5)) ## off diagonals are 0 since PC&#39;s are orthogonal
</code></pre>

<pre><code>## [1] 0
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-34" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">var_explained &lt;- round(eigenvalues/sum(eigenvalues) * 100, digits = 2)
cum_var_explained &lt;- round(cumsum(eigenvalues)/sum(eigenvalues) * 100, digits = 2)

var_explained &lt;- as.data.frame(var_explained)
names(var_explained) &lt;- &quot;variance_explained&quot;
var_explained$PC &lt;- as.numeric(rownames(var_explained))
var_explained &lt;- cbind(var_explained,cum_var_explained)

library(ggplot2)
ggplot(var_explained) +
  geom_bar(aes(x=PC,y=variance_explained),stat=&#39;identity&#39;) +
  geom_line(aes(x=PC,y=cum_var_explained))
</code></pre>

<p><img src="figure/unnamed-chunk-5.png" alt="plot of chunk unnamed-chunk-5"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-35" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">pca.df &lt;- prcomp(scaled_features)
eigenvalues == (pca.df$sdev)^2
</code></pre>

<pre><code>## [1] FALSE FALSE FALSE FALSE
</code></pre>

<pre><code class="r">eigenvectors[,1] == pca.df$rotation[,1]
</code></pre>

<pre><code>##        pH sulphates   alcohol   quality 
##     FALSE     FALSE     FALSE     FALSE
</code></pre>

<pre><code class="r">sum((eigenvectors[,1])^2)
</code></pre>

<pre><code>## [1] 1
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-36" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">rows &lt;- nrow(tennis)
pca.plot &lt;- as.data.frame(pca.df$x[,1:2])
pca.plot$gender &lt;- data$Gender
ggplot(data=pca.plot,aes(x=PC1,y=PC2,color=gender)) + geom_point()
</code></pre>

<pre><code>## Error: object &#39;gender&#39; not found
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-37" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<ul>
<li>how accurate is the first PC at dividing the dataset?
gen &lt;- ifelse(pca.df$x[,1] &gt; abs(mean(pca.df$x[,1]))*2,&quot;F&quot;,&quot;M&quot;)
sum(diag(table(gen,as.character(data$Gender))))/rows</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-38" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Summary</h1>

<p><space></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-39" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>used to separate data into meaningful or useful groups (or both)

<ul>
<li>capture natural structure of the data</li>
<li>useful starting point for further analysis</li>
</ul></li>
<li>customer segmentation</li>
<li>cluster for utility

<ul>
<li>summarizing data for less expensive computation</li>
<li>data compression</li>
<li>nearest neighbors - distance between two cluster centers (centroids)</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-40" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>types of clusters

<ul>
<li>data points that are more similar to one another than points outside of the cluster - most intuitive definition</li>
<li>prototype-based: each data point is more similar to the prototype, i.e. center, of the cluster than the prototype of other clusters. Often a centroid, i.e. mean.</li>
<li>density based clusters: where the density is highest, that is a cluster. Works well for data with noise and outliers. Clusters separated by noise.</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-41" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>example of density based cluster</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-42" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans</h1>

<p><space></p>

<ul>
<li>k-means

<ul>
<li>prototype, partitional based</li>
<li>choose K initial centroids/clusters</li>
<li>points are assigned to the closest centroid</li>
<li>centroid is then updated based on the points in that cluster</li>
<li>update steps until no point changes or centroids remain the same</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-43" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans</h1>

<p><space></p>

<ol>
<li>Select K points as initial centroids. </li>
<li>repeat</li>
<li>    Form K clusters by assigning each point to its closest centroid.</li>
<li>    Recompute the centroid of each cluster. </li>
<li><p>until Centroids do not change, or change very minimally, i.e. &lt;1%</p></li>
<li><p>Use similarity measures such as Euclidean or cosine similarity depending on the data</p></li>
<li><p>Minimize the squared distance of each point to closest centroid, minimize the objective function</p>

<ul>
<li>the centroid that minimizes the SSE of the cluster is the mean</li>
<li>Kmeans leads to local minimum, not global, since you’re optimizing based on the centroids you chose, not all possible centroids</li>
</ul></li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-44" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans</h1>

<p><space></p>

<ul>
<li><p>choose K randomly - can lead to poor centroids</p>

<ul>
<li>run k-means multiple times - still doesn’t solve problems</li>
</ul></li>
<li><p>can reduce the total SSE by increasing the K</p>

<ul>
<li>can increase the cluster with largest SSE</li>
</ul></li>
<li><p>can decrease K and minimize SSE</p>

<ul>
<li>split up a cluster into other clusters. the centroid that is split will increase total SSE the least</li>
</ul></li>
<li><p>bisecting K means</p>

<ul>
<li>less susceptible to initialization problems</li>
<li>split points into 2 clusters

<ul>
<li>take cluster with largest SSE - split that into two clusters</li>
</ul></li>
<li>rerun bisecting K mean on resulting clusters</li>
<li>stop when you have K clusters</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-45" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans</h1>

<p><space></p>

<ul>
<li>K mean fails

<ul>
<li>if some clusters are much bigger than other clusters - it cannot distinguish between natural clusters</li>
<li>if clusters have different densities, K means cannot tell </li>
<li>distance metric doesn’t account for non-globular clusters, i.e. if they follow a distribution</li>
</ul></li>
<li>K means will still work if user accepts sub clusters of the natural cluster</li>
<li>strengths

<ul>
<li>simple, efficient computationally</li>
<li>not useful for non-globular, different density, different sized data</li>
<li>outlier detection and removal can help address outlier problem</li>
</ul></li>
<li>can derive K mean algorithm using gradient descent

<ul>
<li>can use calculus to show that the mean of the cluster is the best choice of centroid, i.e. minimizes SSE</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-46" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans</h1>

<p><space></p>

<p>tennis_kmean &lt;- kmeans(features, centers=5)</p>

<h1>K MEANS DOES A GOOD JOB IN CLUSTERING GENDERS</h1>

<p>table(tennis$Gender,tennis_kmean$cluster)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-47" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans animation</h1>

<p><space></p>

<h1>animate kmean algorithm</h1>

<p>install.packages(&#39;animation&#39;)
library(animation)</p>

<p>oopt = ani.options(interval = 1)</p>

<h2>the kmeans() example; very fast to converge!</h2>

<p>ani_ex = rbind(matrix(rnorm(100, sd = 0.3), ncol = 2), 
          matrix(rnorm(100, sd = 0.3), 
          ncol = 2))
colnames(ani_ex) = c(&quot;x&quot;, &quot;y&quot;)</p>

<p>kmeans.an = function(
  x = cbind(X1 = runif(50), X2 = runif(50)), centers = 4, hints = c(&#39;Move centers!&#39;, &#39;Find cluster?&#39;),
  pch = 1:5, col = 1:5
) {
  x = as.matrix(x)
  ocluster = sample(centers, nrow(x), replace = TRUE)
  if (length(centers) == 1) centers = x[sample(nrow(x), centers), ] else
    centers = as.matrix(centers)
  numcent = nrow(centers)
  dst = matrix(nrow = nrow(x), ncol = numcent)
  j = 1
  pch = rep(pch, length = numcent)
  col = rep(col, length = numcent)</p>

<p>for (j in 1:ani.options(&#39;nmax&#39;)) {
    dev.hold()
    plot(x, pch = pch[ocluster], col = col[ocluster], panel.first = grid())
    mtext(hints[1], 4)
    points(centers, pch = pch[1:numcent], cex = 3, lwd = 2, col = col[1:numcent])
    ani.pause()
    for (i in 1:numcent) {
      dst[, i] = sqrt(apply((t(t(x) - unlist(centers[i, ])))<sup>2,</sup> 1, sum))
    }
    ncluster = apply(dst, 1, which.min)
    plot(x, type = &#39;n&#39;)
    mtext(hints[2], 4)
    grid()
    ocenters = centers
    for (i in 1:numcent) {
      xx = subset(x, ncluster == i)
      polygon(xx[chull(xx), ], density = 10, col = col[i], lty = 2)
      points(xx, pch = pch[i], col = col[i])
      centers[i, ] = apply(xx, 2, mean)
    }
    points(ocenters, cex = 3, col = col[1:numcent], pch = pch[1:numcent], lwd = 2)
    ani.pause()
    if (all(ncluster == ocluster)) break
    ocluster = ncluster
  }
  invisible(list(cluster = ncluster, centers = centers))
}</p>

<p>kmeans.an(ani_ex, centers = 5, hints = c(&quot;Move centers&quot;,&quot;Cluster found?&quot;))</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-48" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>DBSCAN</h1>

<p><space></p>

<ul>
<li>density based

<ul>
<li>center based approach to finding density</li>
<li>count the number of points within some radius of a point, the radius is call Eps</li>
<li>if Eps is too big, there will be m points, if eps is too small, there will be 1 point</li>
<li>core point has X points within a radius of Eps, border points are within a radius of Eps of core point, and noise points are not within Eps of border or core points</li>
<li>if p is density connected to q, they are part of the same cluster, if not, then they are not; if p is not density connected to any other point, its considered noise</li>
</ul></li>
</ul>

<hr>

<h2>Clustering</h2>

<h1>DBSCAN</h1>

<p><space></p>

<pre><code class="r">x &lt;- c(2,2,8,5,7,6,1,4)
y &lt;- c(10,5,4,8,5,4,2,9)
cluster &lt;- data.frame(X=c(x,2*x,3*x),Y=c(y,-2*x,1/4*y))
plot(cluster)
</code></pre>

<p><img src="figure/unnamed-chunk-8.png" alt="plot of chunk unnamed-chunk-8"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-49" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>DBSCAN</h1>

<p><space></p>

<pre><code class="r">library(fpc)
cluster_DBSCAN&lt;-dbscan(cluster, eps=3, MinPts=2, method=&quot;hybrid&quot;)
plot(cluster_DBSCAN, cluster, main=&quot;Clustering using DBSCAN algorithm (eps=3, MinPts=3)&quot;)
</code></pre>

<p><img src="figure/dbscan_ex.png" alt="plot of chunk dbscan_ex"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-50" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Summary</h1>

<p><space></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-51" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>A root node that has no incoming edges and zero or more outgoing edges.</li>
<li>Internal nodes, each of which has exactly one incoming edge and two or more outgoing edges</li>
<li><p>Leaf or terminal nodes, each of which has exactly one incoming edge and no outgoing edges. </p></li>
<li><p>The non- terminal nodes, which include the root and other internal nodes, contain attribute test conditions to separate records that have different characteristics</p></li>
<li><p>trees work best with categorical values</p></li>
<li><p>descriptions are disjoint</p></li>
<li><p>trees are robust to data errors</p></li>
<li><p>training data is missing values</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-52" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Algorithm(s)</h1>

<p><space></p>

<p>ID3
C4.5
C5.0</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-53" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy calculation</h1>

<p><space></p>

<ul>
<li>The entropy of a sample of data indicates how mixed the class values are; the minimum value of 0 indicates that the sample is completely homogenous, while 1 indicates the maximum amount of disorder.</li>
</ul>

<p>entropy_function &lt;- function(p) {</p>

<p>if (min(p) &lt; 0 || sum(p) &lt;= 0) {
    return(NA)
  } else {
    p.norm &lt;- p[p&gt;0]/sum(p)
    -sum(log2(p.norm)*p.norm)
    }
}</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-54" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy calculation</h1>

<p><space></p>

<ul>
<li>InfoGain = Entropy (pre split) - Entropy (post split)

<ul>
<li>Entropy is weighted by the Entropy of each feature split

<ul>
<li>avoid pre-pruning because its impossible to know if the tree will miss subtle but important patterns in the data (if you prune too early)</li>
</ul></li>
</ul></li>
<li>hard to know optimal length of tree without growing it there first</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-55" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy calculation</h1>

<p><space></p>

<ul>
<li>Entropy = expected amount of information contained in a random variable -&gt; information is synonymous with &quot;bits&quot; which is why is log, base 2 

<ul>
<li>the more a feature splits the data in obvious ways, the less informative it is for us, entropy is lower</li>
<li>the more the feature splits the data, the higher the entropy and hence information gained by splitting at that feature</li>
<li>Entropy is minimized when one of the events has a P(X)=1</li>
<li>Entropy is maximized when each event has a P(X)=1/n of happening</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-56" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy calculation</h1>

<p><space></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-57" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">voting_data &lt;- read.csv(&#39;http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data&#39;)
str(voting_data)
names(voting_data) &lt;- c(&#39;party&#39;,&#39;handicapped-infants&#39;,&#39;water-project-cost-sharing&#39;,
                        &#39;adoption-of-the-budget-resolution&#39;,&#39;physician-fee-freeze&#39;,
                        &#39;el-salvador-aid&#39;,&#39;religious-groups-in-schools&#39;,
                        &#39;anti-satellite-test-ban&#39;,&#39;aid-to-nicaraguan-contras&#39;,
                        &#39;mx-missile&#39;,&#39;immigration&#39;,&#39;synfuels-corporation-cutback&#39;,
                        &#39;education-spending&#39;,&#39;superfund-right-to-sue&#39;,&#39;crime&#39;,
                        &#39;duty-free-exports&#39;,&#39;export-administration-act-south-africa&#39;)


prop.table(table(voting_data[,1]))
n &lt;- nrow(voting_data)
train_ind &lt;- sample(n,2/3*n)
voting_train &lt;- voting_data[train_ind,]
voting_test &lt;- voting_data[-train_ind,]
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-58" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code>     party handicapped-infants water-project-cost-sharing
</code></pre>

<p>408   democrat                   y                          n
273 republican                   n                          n
406   democrat                   y                          n
138   democrat                   n                          n
271   democrat                   n                          y
426   democrat                   y                          n
    adoption-of-the-budget-resolution physician-fee-freeze el-salvador-aid
408                                 y                    n               n
273                                 n                    y               y
406                                 y                    n               y
138                                 y                    n               n
271                                 y                    n               n
426                                 y                    n               n
    religious-groups-in-schools anti-satellite-test-ban
408                           y                       y
273                           n                       y
406                           y                       n
138                           y                       y
271                           y                       y
426                           n                       y
    aid-to-nicaraguan-contras mx-missile immigration
408                         y          y           n
273                         y          n           y
406                         n          y           y
138                         y          y           y
271                         y          y           n
426                         y          y           y
    synfuels-corporation-cutback education-spending superfund-right-to-sue
408                            n                  y                      ?
273                            n                  y                      y
406                            n                  n                      y
138                            n                  n                      n
271                            ?                  n                      n
426                            n                  n                      n
    crime duty-free-exports export-administration-act-south-africa
408     y                 y                                      y
273     y                 ?                                      y
406     y                 n                                      y
138     y                 n                                      y
271     n                 n                                      y
426     n                 y                                      y
<img src="figure/tree_plot.png" alt="plot of chunk tree_plot"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-59" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r"># make tree using C5.0
tree_model &lt;- C5.0(voting_train[,-1],voting_train[,1], trials=1)
tree_predict &lt;- predict(tree_model, newdata=voting_test[,-1])
conf &lt;- CrossTable(voting_test[,1], tree_predict, prop.chisq = FALSE,
                   prop.c = FALSE, prop.r = FALSE,
                   dnn = c(&quot;actual class&quot;, &quot;predicted class&quot;))
</code></pre>

<pre><code>## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |         N / Table Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  145 
## 
##  
##              | predicted class 
## actual class |   democrat | republican |  Row Total | 
## -------------|------------|------------|------------|
##     democrat |         89 |          6 |         95 | 
##              |      0.614 |      0.041 |            | 
## -------------|------------|------------|------------|
##   republican |          0 |         50 |         50 | 
##              |      0.000 |      0.345 |            | 
## -------------|------------|------------|------------|
## Column Total |         89 |         56 |        145 | 
## -------------|------------|------------|------------|
## 
## 
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-60" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r"># most important variables
C5imp(tree_model)
</code></pre>

<pre><code>##                                        Overall
## physician-fee-freeze                     96.89
## synfuels-corporation-cutback             42.21
## mx-missile                               10.73
## religious-groups-in-schools               2.08
## handicapped-infants                       0.00
## water-project-cost-sharing                0.00
## adoption-of-the-budget-resolution         0.00
## el-salvador-aid                           0.00
## anti-satellite-test-ban                   0.00
## aid-to-nicaraguan-contras                 0.00
## immigration                               0.00
## education-spending                        0.00
## superfund-right-to-sue                    0.00
## crime                                     0.00
## duty-free-exports                         0.00
## export-administration-act-south-africa    0.00
</code></pre>

<pre><code class="r"># in-sample error rate
summary(tree_model)
</code></pre>

<pre><code>## 
## Call:
## C5.0.default(x = voting_train[, -1], y = voting_train[, 1], trials = 1)
## 
## 
## C5.0 [Release 2.07 GPL Edition]      Mon Aug 11 19:48:29 2014
## -------------------------------
## 
## Class specified by attribute `outcome&#39;
## 
## Read 289 cases (17 attributes) from undefined.data
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (165.1/3.7)
## physician-fee-freeze = y:
## :...synfuels-corporation-cutback in {?,n}: republican (99.2/2.7)
##     synfuels-corporation-cutback = y:
##     :...mx-missile in {?,n}: republican (20/4.3)
##         mx-missile = y:
##         :...religious-groups-in-schools = n: republican (1)
##             religious-groups-in-schools in {?,y}: democrat (3.6)
## 
## 
## Evaluation on training data (289 cases):
## 
##      Decision Tree   
##    ----------------  
##    Size      Errors  
## 
##       5   10( 3.5%)   &lt;&lt;
## 
## 
##     (a)   (b)    &lt;-classified as
##    ----  ----
##     167     5    (a): class democrat
##       5   112    (b): class republican
## 
## 
##  Attribute usage:
## 
##   96.89% physician-fee-freeze
##   42.21% synfuels-corporation-cutback
##   10.73% mx-missile
##    2.08% religious-groups-in-schools
## 
## 
## Time: 0.0 secs
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-61" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example - Boosting</h1>

<p><space></p>

<ul>
<li>rooted in the notion that by combining a number of weak performing learners, </li>
<li>you can create a team that is much stronger than any one of the learners alone.</li>
<li>this is where C5.0 improves on C4.5</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-62" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example - Boosting</h1>

<p><space></p>

<pre><code class="r">boosted_tree_model &lt;- C5.0(voting_train[,-1],voting_train[,1], trials=25)
boosted_tennis_predict &lt;- predict(boosted_tree_model,voting_test[,-1])

boosted_conf &lt;- CrossTable(voting_test[,1], boosted_tennis_predict, prop.chisq = FALSE,
                           prop.c = FALSE, prop.r = FALSE, 
                           dnn = c(&quot;actual class&quot;, &quot;predicted class&quot;))
</code></pre>

<p>Cell Contents
|-------------------------|
|                       N |
|         N / Table Total |
|-------------------------|</p>

<p>Total Observations in Table:  145 </p>

<pre><code>         | predicted class 
</code></pre>

<table><thead>
<tr>
<th>actual class</th>
<th>democrat</th>
<th>republican</th>
<th>Row Total</th>
</tr>
</thead><tbody>
<tr>
<td>democrat</td>
<td>89</td>
<td>6</td>
<td>95</td>
</tr>
<tr>
<td></td>
<td>0.614</td>
<td>0.041</td>
<td></td>
</tr>
<tr>
<td>-------------</td>
<td>------------</td>
<td>------------</td>
<td>------------</td>
</tr>
<tr>
<td>republican</td>
<td>0</td>
<td>50</td>
<td>50</td>
</tr>
<tr>
<td></td>
<td>0.000</td>
<td>0.345</td>
<td></td>
</tr>
<tr>
<td>-------------</td>
<td>------------</td>
<td>------------</td>
<td>------------</td>
</tr>
<tr>
<td>Column Total</td>
<td>89</td>
<td>56</td>
<td>145</td>
</tr>
<tr>
<td>-------------</td>
<td>------------</td>
<td>------------</td>
<td>------------</td>
</tr>
</tbody></table>

<pre><code class="r"># in-sample error rate
summary(boosted_tree_model)
</code></pre>

<p>Call:
C5.0.default(x = voting_train[, -1], y = voting_train[, 1], trials = 25)</p>

<h2>C5.0 [Release 2.07 GPL Edition]     Mon Aug 11 19:48:29 2014</h2>

<p>Class specified by attribute `outcome&#39;</p>

<p>Read 289 cases (17 attributes) from undefined.data</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-63" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>physician-fee-freeze in {?,n}: democrat (165.1/3.7)
physician-fee-freeze = y:
:...synfuels-corporation-cutback in {?,n}: republican (99.2/2.7)
    synfuels-corporation-cutback = y:
    :...mx-missile in {?,n}: republican (20/4.3)
        mx-missile = y:
        :...religious-groups-in-schools = n: republican (1)
            religious-groups-in-schools in {?,y}: democrat (3.6)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-64" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>adoption-of-the-budget-resolution in {?,y}: democrat (149.3/21.5)
adoption-of-the-budget-resolution = n:
:...synfuels-corporation-cutback in {?,n}: republican (100.7/11.7)
    synfuels-corporation-cutback = y: democrat (39/13.1)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-65" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>physician-fee-freeze = n: democrat (128.5/23.2)
physician-fee-freeze in {?,y}: republican (160.5/37.7)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-66" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>crime in {?,n}: democrat (77.4/9.4)
crime = y:
:...synfuels-corporation-cutback in {?,n}: republican (121.6/38.3)
    synfuels-corporation-cutback = y: democrat (90.1/33.1)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-67" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>adoption-of-the-budget-resolution in {?,y}: democrat (139/29.9)
adoption-of-the-budget-resolution = n:
:...education-spending = n: democrat (56/19.9)
    education-spending in {?,y}: republican (94/16.5)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-68" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>physician-fee-freeze in {?,n}: democrat (113.4/24)
physician-fee-freeze = y:
:...immigration = n: democrat (98/46.3)
    immigration in {?,y}: republican (77.6/5.2)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-69" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>physician-fee-freeze = ?: republican (0)
physician-fee-freeze = n: democrat (100.5/24.9)
physician-fee-freeze = y:
:...immigration in {?,y}: republican (67.9/6.2)
    immigration = n:
    :...adoption-of-the-budget-resolution in {?,n}: republican (93.9/27.2)
        adoption-of-the-budget-resolution = y: democrat (26.7/9.6)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-70" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>synfuels-corporation-cutback = ?: republican (0)
synfuels-corporation-cutback = n:
:...duty-free-exports in {?,n}: republican (115.7/24.2)
:   duty-free-exports = y: democrat (57.1/22.5)
synfuels-corporation-cutback = y:
:...physician-fee-freeze in {?,n}: democrat (28.5/1.5)
    physician-fee-freeze = y:
    :...mx-missile = n: republican (62.2/27.3)
        mx-missile in {?,y}: democrat (25.5/4.1)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-71" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>physician-fee-freeze = ?: democrat (0)
physician-fee-freeze = n:
:...adoption-of-the-budget-resolution = n: republican (40/15.8)
:   adoption-of-the-budget-resolution in {?,y}: democrat (70.8/4.8)
physician-fee-freeze = y:
:...synfuels-corporation-cutback in {?,n}: republican (97.9/20.6)
    synfuels-corporation-cutback = y:
    :...immigration in {?,n}: democrat (56.3/17.2)
        immigration = y: republican (24/7.7)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-72" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>physician-fee-freeze in {?,n}: democrat (118.1/26.5)
physician-fee-freeze = y:
:...superfund-right-to-sue in {?,n}: republican (21.4/2.4)
    superfund-right-to-sue = y:
    :...education-spending = ?: republican (0)
        education-spending = n: democrat (45.9/15.5)
        education-spending = y:
        :...anti-satellite-test-ban in {?,y}: republican (16.9/0.2)
            anti-satellite-test-ban = n:
            :...adoption-of-the-budget-resolution in {?,
                :                                     n}: republican (60.7/12.2)
                adoption-of-the-budget-resolution = y: democrat (26/6.3)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-73" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>physician-fee-freeze = ?: republican (0)
physician-fee-freeze = n: democrat (109.7/33.3)
physician-fee-freeze = y:
:...synfuels-corporation-cutback in {?,n}: republican (96.8/19.4)
    synfuels-corporation-cutback = y:
    :...mx-missile = n: republican (60.6/25.5)
        mx-missile in {?,y}: democrat (21.9/4)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-74" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>crime in {?,n}: democrat (40.2/8.8)
crime = y:
:...anti-satellite-test-ban in {?,y}: republican (101.1/26.2)
    anti-satellite-test-ban = n:
    :...physician-fee-freeze in {?,n}: democrat (15.2/0.9)
        physician-fee-freeze = y:
        :...immigration in {?,n}: democrat (109.2/39.4)
            immigration = y: republican (23.3/4.6)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-75" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>adoption-of-the-budget-resolution in {?,y}: democrat (111.5/36.6)
adoption-of-the-budget-resolution = n:
:...synfuels-corporation-cutback in {?,n}: republican (102.3/24.6)
    synfuels-corporation-cutback = y: democrat (75.2/30.1)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-76" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>physician-fee-freeze = ?: republican (0)
physician-fee-freeze = n: democrat (99/31.9)
physician-fee-freeze = y:
:...immigration in {?,y}: republican (61/8.4)
    immigration = n:
    :...duty-free-exports in {?,n}: republican (106.1/40.9)
        duty-free-exports = y: democrat (22.8/6.6)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-77" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>adoption-of-the-budget-resolution = ?: republican (0)
adoption-of-the-budget-resolution = y: democrat (110.5/39.1)
adoption-of-the-budget-resolution = n:
:...synfuels-corporation-cutback in {?,n}: republican (94.1/25.9)
    synfuels-corporation-cutback = y: democrat (84.3/38.3)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-78" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>physician-fee-freeze = ?: republican (0)
physician-fee-freeze = n: democrat (88.6/30.5)
physician-fee-freeze = y:
:...immigration in {?,y}: republican (67.7/13)
    immigration = n:
    :...anti-satellite-test-ban in {?,y}: republican (16.6/1.9)
        anti-satellite-test-ban = n:
        :...adoption-of-the-budget-resolution in {?,y}: democrat (23.4/1.7)
            adoption-of-the-budget-resolution = n:
            :...education-spending = n: democrat (39.7/14.9)
                education-spending in {?,y}: republican (53/16.3)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-79" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>mx-missile = ?: republican (0)
mx-missile = y:
:...religious-groups-in-schools = n: republican (49.9/20.4)
:   religious-groups-in-schools in {?,y}: democrat (63.6/11.8)
mx-missile = n:
:...el-salvador-aid = ?: republican (0)
    el-salvador-aid = n: democrat (5/0.2)
    el-salvador-aid = y:
    :...anti-satellite-test-ban in {?,y}: republican (37.6/1.2)
        anti-satellite-test-ban = n:
        :...immigration in {?,y}: republican (24.6/2.1)
            immigration = n:
            :...adoption-of-the-budget-resolution in {?,
                :                                     y}: democrat (18.1/0.7)
                adoption-of-the-budget-resolution = n:
                :...physician-fee-freeze = n: democrat (3.8/0.1)
                    physician-fee-freeze in {?,y}: republican (86.5/35.2)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-80" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>synfuels-corporation-cutback = ?: republican (0)
synfuels-corporation-cutback = n:
:...el-salvador-aid = n: democrat (74.7/26.2)
:   el-salvador-aid in {?,y}: republican (81.3/22.6)
synfuels-corporation-cutback = y:
:...physician-fee-freeze in {?,n}: democrat (35.6/1.1)
    physician-fee-freeze = y:
    :...superfund-right-to-sue = n: republican (7.1/0.9)
        superfund-right-to-sue in {?,y}: democrat (90.3/36.6)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-81" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>adoption-of-the-budget-resolution in {?,n}: republican (170.5/65.8)
adoption-of-the-budget-resolution = y: democrat (118.5/34.2)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-82" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>physician-fee-freeze in {?,n}: democrat (113.3/30)
physician-fee-freeze = y:
:...immigration in {?,y}: republican (59.7/10.8)
    immigration = n:
    :...export-administration-act-south-africa = n: democrat (37.4/12.8)
        export-administration-act-south-africa in {?,y}: republican (78.6/32.2)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-83" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>physician-fee-freeze = ?: republican (0)
physician-fee-freeze = n: democrat (109.8/34.7)
physician-fee-freeze = y:
:...synfuels-corporation-cutback in {?,n}: republican (78.6/11.6)
    synfuels-corporation-cutback = y:
    :...mx-missile = n: republican (74.4/30.2)
        mx-missile in {?,y}: democrat (25.2/5.4)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-84" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>synfuels-corporation-cutback = ?: republican (0)
synfuels-corporation-cutback = n:
:...adoption-of-the-budget-resolution in {?,n}: republican (85.9/13.3)
:   adoption-of-the-budget-resolution = y: democrat (64.2/25.4)
synfuels-corporation-cutback = y:
:...physician-fee-freeze in {?,n}: democrat (25.9/1.1)
    physician-fee-freeze = y:
    :...superfund-right-to-sue = ?: democrat (0)
        superfund-right-to-sue = n: republican (12.3/1.2)
        superfund-right-to-sue = y:
        :...mx-missile in {?,y}: democrat (17.6/0.7)
            mx-missile = n:
            :...immigration in {?,n}: democrat (70.2/23.9)
                immigration = y: republican (11.9)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-85" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>crime = ?: republican (0)
crime = n: democrat (45.5/9.5)
crime = y:
:...superfund-right-to-sue in {?,n}: republican (66.7/13.7)
    superfund-right-to-sue = y:
    :...physician-fee-freeze = ?: republican (0)
        physician-fee-freeze = n: democrat (19.2/0.7)
        physician-fee-freeze = y:
        :...duty-free-exports in {?,y}: republican (12.8/0.1)
            duty-free-exports = n:
            :...el-salvador-aid = ?: republican (0)
                el-salvador-aid = n: democrat (7.3/2)
                el-salvador-aid = y:
                :...anti-satellite-test-ban in {?,y}: republican (21.4/0.2)
                    anti-satellite-test-ban = n: [S1]</p>

<p>SubTree [S1]</p>

<p>adoption-of-the-budget-resolution in {?,n}: republican (83.9/29.1)
adoption-of-the-budget-resolution = y: democrat (31.1/5.4)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-86" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>physician-fee-freeze = ?: republican (0)
physician-fee-freeze = n: democrat (127.3/32)
physician-fee-freeze = y:
:...water-project-cost-sharing in {?,n}: republican (39.6/1.3)
    water-project-cost-sharing = y:
    :...immigration in {?,y}: republican (36.8/6.4)
        immigration = n:
        :...adoption-of-the-budget-resolution in {?,n}: republican (61.3/20.1)
            adoption-of-the-budget-resolution = y: democrat (22/1.6)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-87" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p>Decision tree:</p>

<p>mx-missile in {?,y}: democrat (135.8/22.4)
mx-missile = n:
:...handicapped-infants in {?,n}: republican (88.9/16.1)
    handicapped-infants = y: democrat (60.3/18.7)</p>

<p>Evaluation on training data (289 cases):</p>

<p>Trial       Decision Tree   </p>

<hr>

<pre><code>  Size      Errors  
</code></pre>

<p>0         5   10( 3.5%)
   1         3   34(11.8%)
   2         2   16( 5.5%)
   3         3   42(14.5%)
   4         3   29(10.0%)
   5         3   50(17.3%)
   6         4   13( 4.5%)
   7         5   41(14.2%)
   8         5   28( 9.7%)
   9         6   16( 5.5%)
  10         4    8( 2.8%)
  11         5   73(25.3%)
  12         3   34(11.8%)
  13         4   14( 4.8%)
  14         3   34(11.8%)
  15         6   12( 4.2%)
  16         8   96(33.2%)
  17         5   31(10.7%)
  18         2   35(12.1%)
  19         4   24( 8.3%)
  20         4    8( 2.8%)
  21         7   23( 8.0%)
  22         8   38(13.1%)
  23         5   11( 3.8%)
  24         3   50(17.3%)
boost             6( 2.1%)   &lt;&lt;</p>

<pre><code>   (a)   (b)    &lt;-classified as
  ----  ----
   170     2    (a): class democrat
     4   113    (b): class republican


Attribute usage:

 96.89% physician-fee-freeze
 96.54% adoption-of-the-budget-resolution
 96.19% synfuels-corporation-cutback
 95.85% mx-missile
 95.50% crime
 78.89% el-salvador-aid
 65.05% duty-free-exports
 63.32% anti-satellite-test-ban
 59.86% superfund-right-to-sue
 51.90% handicapped-infants
 49.83% religious-groups-in-schools
 49.13% immigration
 45.33% education-spending
 38.41% water-project-cost-sharing
 14.88% export-administration-act-south-africa
</code></pre>

<p>Time: 0.0 secs</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-88" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example - Error Cost</h1>

<p><space></p>

<ul>
<li>still getting too many false positives (predict republican but actually democrat)</li>
<li>introduce higher cost to getting this wrong</li>
</ul>

<pre><code class="r">error_cost &lt;- matrix(c(0,1,2,0),nrow=2)
cost_model &lt;- C5.0(voting_train[,-1],voting_train[,1], trials=1, costs = error_cost)
</code></pre>

<pre><code>## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
</code></pre>

<pre><code class="r">cost_predict &lt;- predict(cost_model, newdata=voting_test[,-1])
conf &lt;- CrossTable(voting_test[,1], cost_predict, prop.chisq = FALSE,
                   prop.c = FALSE, prop.r = FALSE,
                   dnn = c(&quot;actual class&quot;, &quot;predicted class&quot;))
</code></pre>

<pre><code>## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |         N / Table Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  145 
## 
##  
##              | predicted class 
## actual class |   democrat | republican |  Row Total | 
## -------------|------------|------------|------------|
##     democrat |         87 |          8 |         95 | 
##              |      0.600 |      0.055 |            | 
## -------------|------------|------------|------------|
##   republican |          0 |         50 |         50 | 
##              |      0.000 |      0.345 |            | 
## -------------|------------|------------|------------|
## Column Total |         87 |         58 |        145 | 
## -------------|------------|------------|------------|
## 
## 
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-89" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example - Error Cost</h1>

<p><space></p>

<pre><code class="r">tris &lt;- seq(1,50,by=2)
boost_acc &lt;- NULL
for (i in tris){  
  temp &lt;- C5.0(voting_train[,-1],voting_train[,1], trials=i, costs = error_cost)
  temp_pred &lt;- predict(temp,voting_test[,-1])
  boost_acc &lt;- append(boost_acc,sum(diag(table(temp_pred,voting_test[,1]))))
}
</code></pre>

<pre><code>## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
</code></pre>

<pre><code class="r">plot(boost_acc,type=&#39;l&#39;)
</code></pre>

<p><img src="figure/unnamed-chunk-10.png" alt="plot of chunk unnamed-chunk-10"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-90" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Pros and Cons</h1>

<p><space></p>

<ul>
<li>trees are non-parametric, rule based classification or regression method</li>
<li>simple to understand and interpret</li>
<li>little data preparation</li>
<li>easy to overfit (need to prune to avoid that, or have max tree depth)</li>
<li>usually finds local optimum. Can mitigate this with an ensemble of trees</li>
<li>difficult concepts that are not easily expressed by trees (XOR) are hard to learn</li>
<li>for class imbalance, trees can be biased - should balance dataset before fitting</li>
<li>trees tend to overfit, so use PCA beforehand</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-91" style="background:;">
  <hgroup>
    <h2>Missing Data</h2>
  </hgroup>
  <article>
    <h1>Types</h1>

<p><space></p>

<p>Missingness that...</p>

<ul>
<li>is completely at random; no bias in missing data</li>
<li>is random</li>
<li>depends on unobserved features</li>
<li><p>depends on the missing value itself</p>

<h2><a href="http://www.stat.columbia.edu/%7Egelman/arm/missing.pdf">http://www.stat.columbia.edu/~gelman/arm/missing.pdf</a></h2>

<h2>Resources</h2>

<p><space></p></li>
<li><p><a href="http://www.packtpub.com/machine-learning-with-r/book">Machine Learning with R</a></p></li>
<li><p><a href="http://shop.oreilly.com/product/0636920018483.do">Machine Learning for Hackers</a></p></li>
<li><p><a href="http://web.stanford.edu/%7Ehastie/local.ftp/Springer/OLD/ESLII_print4.pdf">Elements of Statistical Learning</a></p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-92" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    
  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>

  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
<!-- Grab CDN jQuery, fall back to local if offline -->
<script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
<script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery-1.7.min.js"><\/script>')</script>
<!-- Load Javascripts for Widgets -->
<!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->
<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
<script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- DONE LOADING HIGHLIGHTER JS FILES -->
</html>