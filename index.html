<!DOCTYPE html>
<html>
<head>
  <title>Machine Learning with R - Part 2</title>
  <meta charset="utf-8">
  <meta name="description" content="Machine Learning with R - Part 2">
  <meta name="author" content="Ilan Man">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
    
</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
    <!-- END LOGO SLIDE -->
    

    <!-- TITLE SLIDE -->
    <!-- Should I move this to a Local Layout File? -->
    <slide class="title-slide segue nobackground">
      <hgroup class="auto-fadein">
        <h1>Machine Learning with R - Part 2</h1>
        <h2></h2>
        <p>Ilan Man<br/>Strategy Operations  @ Squarespace</p>
      </hgroup>
          </slide>

    <!-- SLIDES -->
      <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Agenda</h2>
  </hgroup>
  <article>
    <p><space></p>

<ol>
<li>Logistic Regression</li>
<li>Principle Component Analysis</li>
<li>Clustering</li>
<li>Trees</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Objectives</h2>
  </hgroup>
  <article>
    <p><space></p>

<ol>
<li>Understand some popular algorithms and techniques</li>
<li>Learn how to tune parameters</li>
<li>Practice R</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<pre><code class="r">library(&quot;MASS&quot;)
data(menarche)
log_data &lt;- data.frame(Y=menarche$Menarche/menarche$Total)
log_data$X &lt;- menarche$Age

glm.out &lt; glm(cbind(Menarche, Total-Menarche) ~ Age,family=binomial(logit), data=menarche)
</code></pre>

<pre><code>## Error: object &#39;glm.out&#39; not found
</code></pre>

<pre><code class="r">lm.out &lt;- lm(Y~X, data=log_data)

log_data$fitted &lt;- glm.out$fitted
</code></pre>

<pre><code>## Error: object &#39;glm.out&#39; not found
</code></pre>

<pre><code class="r">data_points &lt;- ggplot(log_data) + geom_point(aes(x=X,y=Y),color=&#39;blue&#39;,size=3)
</code></pre>

<pre><code>## Error: could not find function &quot;ggplot&quot;
</code></pre>

<pre><code class="r">line_points &lt;- data_points + geom_abline(intercept = coef(lm.out)[1], slope = coef(lm.out)[2],color=&#39;green&#39;,size=1)
</code></pre>

<pre><code>## Error: object &#39;data_points&#39; not found
</code></pre>

<pre><code class="r">curve_points &lt;- line_points + geom_line(aes(x=X,y=fitted),color=&#39;red&#39;,size=1) 
</code></pre>

<pre><code>## Error: object &#39;line_points&#39; not found
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Notation</h1>

<p><space></p>

<ul>
<li>introduce notation: hypothesis function, cost function, objective function, sigmoid</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<h1>logistic function - odds ratio - log odds</h1>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code>## Error: could not find function &quot;ggplot&quot;
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code class="r">x &lt;- cbind(1,x)  #Add ones to x  
theta&lt;- c(0,0)  # initalize theta vector 
m &lt;- nrow(x)  # Number of the observations 
grad_cost &lt;- function(X,y,theta) return(sum(((X%*%theta)- y)^2))
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code class="r">gradDescent&lt;-function(X,y,theta,iterations,alpha){
  m &lt;- length(y)
  grad &lt;- rep(0,length(theta))
  cost.df &lt;- data.frame(cost=0,theta=0)

  for (i in 1:iterations){
    h &lt;- X%*%theta
    grad &lt;-  (t(X)%*%(h - y))/m
    theta &lt;- theta - alpha * grad
    cost.df &lt;- rbind(cost.df,c(grad_cost(X,y,theta),theta))    
  }  

  return(list(theta,cost.df))
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code class="r">## initialize X, y and theta
X1&lt;-matrix(ncol=1,nrow=nrow(df),cbind(1,df$X))
Y1&lt;-matrix(ncol=1,nrow=nrow(df),df$Y)

init_theta&lt;-as.matrix(c(0))
grad_cost(X1,Y1,init_theta)
</code></pre>

<pre><code>[1] 5256
</code></pre>

<pre><code class="r">iterations = 100
alpha = 0.1
results &lt;- gradDescent(X1,Y1,init_theta,iterations,alpha)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code>## Error: could not find function &quot;ggplot&quot;
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code class="r">grad_cost(X1,Y1,theta[[1]])
</code></pre>

<pre><code>[1] 314.6
</code></pre>

<pre><code class="r">## Make some predictions
intercept &lt;- df[df$X==0,]$Y
pred &lt;- function (x) return(intercept+c(x)%*%theta)
new_points &lt;- c(0.1,0.5,0.8,1.1)
new_preds &lt;- data.frame(X=new_points,Y=sapply(new_points,pred))
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent</h1>

<p><space></p>

<pre><code class="r">ggplot(data=df,aes(x=X,y=Y))+geom_point(size=2)
</code></pre>

<pre><code>## Error: could not find function &quot;ggplot&quot;
</code></pre>

<pre><code class="r">ggplot(data=df,aes(x=X,y=Y))+geom_point()+geom_point(data=new_preds,aes(x=X,y=Y,color=&#39;red&#39;),size=3)+scale_colour_discrete(guide = FALSE)
</code></pre>

<pre><code>## Error: could not find function &quot;ggplot&quot;
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>Regression example</h2>
  </hgroup>
  <article>
    <h1>Gradient descent - summary</h1>

<p><space></p>

<ul>
<li>minimization algorithm</li>
<li>approximation, non-closed form solution</li>
<li>good for large number of examples</li>
<li>hard to select the right \(\alpha\)</li>
<li>traditional looping is slow - optimization algorithms are used in practice</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<h1>example</h1>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<h1>Summary</h1>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<h1>motivation for PCA</h1>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<h1>brief overview of important linear algebra theorems</h1>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<h1>where L is our eigenvalues and x is eigenvectors and A is our square matrix</h1>

<h1>Ax = Lx</h1>

<h1>Ax - LIx = 0</h1>

<h1>(A-LI)x = 0</h1>

<h1>what is x such that x is not all zero?</h1>

<h1>determinant of A - LI must be 0</h1>

<h1>the solution</h1>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p>A = matrix(c(5,2,2,5),nrow=2)
|A - L<em>diag(nrow(A))| = 0
det(c(5-l,2,2,5-l))
(5-l)</em>(5-l) - 4 = 0
25 - 10l + l<sup>2</sup> - 4 = 0
l<sup>2</sup> - 10l + 21 = 0
roots &lt;- Re(polyroot(c(21,-10,1)))</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<h1>when lambda = -3</h1>

<p>Ax = 3x
5x1 + 2x2 = 3x1
2x1 + 5x2 = 3x2
x1=-x2</p>

<h1>one eigenvector = [1 -1]. any scalar multiple of this counts.</h1>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<h1>when lambda = 7</h1>

<p>5x1 + 2x2 = 7x1
2x2 + 5x2 = 7x2
x1 = x2</p>

<h1>another eigenvector = [1 1]. any scalar multiple of this counts.</h1>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-23" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p>A%<em>%c(1,-1) == 3 * as.matrix(c(1,-1))
A%</em>%c(1,1) == 7 * as.matrix(c(1,1))
roots</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-24" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>check
m &lt;- matrix(c(1,-1,1,1),ncol=2)
m &lt;- m/sqrt(norm(m))
A == as.matrix(m%<em>%diag(roots)%</em>%t(m))
# lambda is a diagonal matrix, with 0 off diagonals</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-25" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p>PX = Y</p>

<p>CY = (1/(n-1))*YYt
=PX(PX)t
=PXXtPt
=PAPt</p>

<h1>P is a matrix with columns that are eigenvectors</h1>

<h1>A is a diagonalized matrix of eigenvalues (by linear algebra) and symmetric</h1>

<p>A = EDEt</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-26" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<h1>each row of P should be an eigenvector of A</h1>

<p>P=Et</p>

<h1>also note that Pt = P-1 (linear algebra)</h1>

<p>A = PtDP
CY = PPtDPPt
= (1/(n-1))*D</p>

<h1>D is a diagonal matrix, depending on how we choose P</h1>

<h1>therefore CY is diagonalized</h1>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-27" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<p>data &lt;- read.csv(&#39;tennis_data_2013.csv&#39;)
data$Player1 &lt;- as.character(data$Player1)
data$Player2 &lt;- as.character(data$Player2)</p>

<p>tennis &lt;- data
m &lt;- length(data)</p>

<p>for (i in 10:m){
  tennis[,i] &lt;- ifelse(is.na(data[,i]),0,data[,i])
}</p>

<p>str(tennis)</p>

<p>features &lt;- tennis[,10:m]</p>

<p>head(features)
str(features)
dim(features)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-28" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<p>scaled_features &lt;- as.matrix(scale(features))
Cx &lt;- cov(scaled_features)
eigenvalues &lt;- eigen(Cx)$values
eigenvectors &lt;- eigen(Cx)$vectors
PC &lt;- scaled_features %*% eigenvectors</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-29" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<p>Cy &lt;- cov(PC)
sum(round(diag(Cy) - eigenvalues,5))
sum(round(Cy[upper.tri(Cy)],5)) ## off diagonals are 0 since PC&#39;s are orthogonal</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-30" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<p>var_explained &lt;- round(eigenvalues/sum(eigenvalues) * 100, digits = 2)
cum_var_explained &lt;- round(cumsum(eigenvalues)/sum(eigenvalues) * 100, digits = 2)</p>

<p>var_explained &lt;- as.data.frame(var_explained)
names(var_explained) &lt;- &quot;variance_explained&quot;
var_explained$PC &lt;- as.numeric(rownames(var_explained))
var_explained &lt;- cbind(var_explained,cum_var_explained)</p>

<p>library(ggplot2)
ggplot(var_explained) +
  geom_bar(aes(x=PC,y=variance_explained),stat=&#39;identity&#39;) +
  geom_line(aes(x=PC,y=cum_var_explained))</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-31" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<p>eigenvalues = (pca.df$sdev)<sup>2</sup>
eigenvectors are the loadings - linear combination of the variables
eigenvectors[,1] = pca.df$rotation[,1]
sum((eigenvectors[,1])<sup>2)</sup></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-32" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<p>pca.df &lt;- prcomp(scaled_features)
rows &lt;- nrow(tennis)
pca.plot &lt;- as.data.frame(pca.df$x[,1:2])
pca.plot$gender &lt;- data$Gender
ggplot(data=pca.plot,aes(x=PC1,y=PC2,color=gender)) + geom_point()</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-33" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<ul>
<li>how accurate is the first PC at dividing the dataset?
gen &lt;- ifelse(pca.df$x[,1] &gt; abs(mean(pca.df$x[,1]))*2,&quot;F&quot;,&quot;M&quot;)
sum(diag(table(gen,as.character(data$Gender))))/rows</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-34" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Summary</h1>

<p><space></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-35" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>used to separate data into meaningful or useful groups (or both)

<ul>
<li>capture natural structure of the data</li>
<li>useful starting point for further analysis</li>
</ul></li>
<li>customer segmentation</li>
<li><p>cluster for utility</p>

<ul>
<li>summarizing data for less expensive computation</li>
<li>data compression</li>
<li>nearest neighbors - distance between two cluster centers (centroids)</li>
</ul></li>
<li><p>types of clusters</p>

<ul>
<li>data points that are more similar to one another than points outside of the cluster - most intuitive definition</li>
<li>prototype-based: each data point is more similar to the prototype, i.e. center, of the cluster than the prototype of other clusters. Often a centroid, i.e. mean.</li>
<li>density based clusters: where the density is highest, that is a cluster. Works well for data with noise and outliers. Clusters separated by noise.</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-36" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans</h1>

<p><space></p>

<ul>
<li>k-means

<ul>
<li>prototype, partitional based</li>
<li>choose K initial centroids/clusters</li>
<li>points are assigned to the closest centroid</li>
<li>centroid is then updated based on the points in that cluster</li>
<li>update steps until no point changes or centroids remain the same</li>
</ul></li>
</ul>

<ol>
<li>Select K points as initial centroids. </li>
<li>repeat</li>
<li>    Form K clusters by assigning each point to its closest centroid.</li>
<li>    Recompute the centroid of each cluster. </li>
<li><p>until Centroids do not change, or change very minimally, i.e. &lt;1%</p></li>
<li><p>Use similarity measures such as Euclidean or cosine similarity depending on the data</p></li>
<li><p>Minimize the squared distance of each point to closest centroid, minimize the objective function</p>

<ul>
<li>the centroid that minimizes the SSE of the cluster is the mean</li>
<li>Kmeans leads to local minimum, not global, since you’re optimizing based on the centroids you chose, not all possible centroids</li>
<li></li>
</ul></li>
<li><p>choose K randomly - can lead to poor centroids</p>

<ul>
<li>run k-means multiple times - still doesn’t solve problems</li>
</ul></li>
</ol>

<ul>
<li>can reduce the total SSE by increasing the K

<ul>
<li>can increase the cluster with largest SSE</li>
</ul></li>
<li>can decrease K and minimize SSE

<ul>
<li>split up a cluster into other clusters. the centroid that is split will increase total SSE the least</li>
</ul></li>
<li><p>bisecting K means</p>

<ul>
<li>less susceptible to initialization problems</li>
<li>split points into 2 clusters

<ul>
<li>take cluster with largest SSE - split that into two clusters</li>
</ul></li>
<li>rerun bisecting K mean on resulting clusters</li>
<li>stop when you have K clusters</li>
</ul></li>
<li><p>K mean fails</p>

<ul>
<li>if some clusters are much bigger than other clusters - it cannot distinguish between natural clusters</li>
<li>if clusters have different densities, K means cannot tell </li>
<li>distance metric doesn’t account for non-globular clusters, i.e. if they follow a distribution</li>
</ul></li>
<li><p>K means will still work if user accepts sub clusters of the natural cluster</p></li>
<li><p>strengths</p>

<ul>
<li>simple, efficient computationally</li>
<li>not useful for non-globular, different density, different sized data</li>
<li>outlier detection and removal can help address outlier problem</li>
</ul></li>
<li><p>can derive K mean algorithm using gradient descent</p>

<ul>
<li>can use calculus to show that the mean of the cluster is the best choice of centroid, i.e. minimizes SSE</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-37" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans</h1>

<p><space></p>

<p>tennis_kmean &lt;- kmeans(features, centers=5)</p>

<h1>K MEANS DOES A GOOD JOB IN CLUSTERING GENDERS</h1>

<p>table(tennis$Gender,tennis_kmean$cluster)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-38" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans animation</h1>

<p><space></p>

<h1>animate kmean algorithm</h1>

<p>install.packages(&#39;animation&#39;)
library(animation)</p>

<p>oopt = ani.options(interval = 1)</p>

<h2>the kmeans() example; very fast to converge!</h2>

<p>ani_ex = rbind(matrix(rnorm(100, sd = 0.3), ncol = 2), 
          matrix(rnorm(100, sd = 0.3), 
          ncol = 2))
colnames(ani_ex) = c(&quot;x&quot;, &quot;y&quot;)</p>

<p>kmeans.an = function(
  x = cbind(X1 = runif(50), X2 = runif(50)), centers = 4, hints = c(&#39;Move centers!&#39;, &#39;Find cluster?&#39;),
  pch = 1:5, col = 1:5
) {
  x = as.matrix(x)
  ocluster = sample(centers, nrow(x), replace = TRUE)
  if (length(centers) == 1) centers = x[sample(nrow(x), centers), ] else
    centers = as.matrix(centers)
  numcent = nrow(centers)
  dst = matrix(nrow = nrow(x), ncol = numcent)
  j = 1
  pch = rep(pch, length = numcent)
  col = rep(col, length = numcent)</p>

<p>for (j in 1:ani.options(&#39;nmax&#39;)) {
    dev.hold()
    plot(x, pch = pch[ocluster], col = col[ocluster], panel.first = grid())
    mtext(hints[1], 4)
    points(centers, pch = pch[1:numcent], cex = 3, lwd = 2, col = col[1:numcent])
    ani.pause()
    for (i in 1:numcent) {
      dst[, i] = sqrt(apply((t(t(x) - unlist(centers[i, ])))<sup>2,</sup> 1, sum))
    }
    ncluster = apply(dst, 1, which.min)
    plot(x, type = &#39;n&#39;)
    mtext(hints[2], 4)
    grid()
    ocenters = centers
    for (i in 1:numcent) {
      xx = subset(x, ncluster == i)
      polygon(xx[chull(xx), ], density = 10, col = col[i], lty = 2)
      points(xx, pch = pch[i], col = col[i])
      centers[i, ] = apply(xx, 2, mean)
    }
    points(ocenters, cex = 3, col = col[1:numcent], pch = pch[1:numcent], lwd = 2)
    ani.pause()
    if (all(ncluster == ocluster)) break
    ocluster = ncluster
  }
  invisible(list(cluster = ncluster, centers = centers))
}</p>

<p>kmeans.an(ani_ex, centers = 5, hints = c(&quot;Move centers&quot;,&quot;Cluster found?&quot;))</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-39" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>DBSCAN</h1>

<p><space></p>

<ul>
<li>density based

<ul>
<li>center based approach to finding density</li>
<li>count the number of points within some radius of a point, the radius is call Eps</li>
<li>if Eps is too big, there will be m points, if eps is too small, there will be 1 point</li>
<li>core point has X points within a radius of Eps, border points are within a radius of Eps of core point, and noise points are not within Eps of border or core points</li>
<li>if p is density connected to q, they are part of the same cluster, if not, then they are not; if p is not density connected to any other point, its considered noise</li>
</ul></li>
</ul>

<hr>

<h2>Clustering</h2>

<h1>DBSCAN</h1>

<p><space></p>

<p>x &lt;- c(2,2,8,5,7,6,1,4)
y &lt;- c(10,5,4,8,5,4,2,9)
cluster &lt;- data.frame(X=c(x,2*x,3*x),Y=c(y,-2*x,1/4*y))
plot(cluster)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-40" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>DBSCAN</h1>

<p><space></p>

<p>install.packages(&#39;fpc&#39;)
library(fpc)
cluster_DBSCAN&lt;-dbscan(cluster, eps=3, MinPts=2, method=&quot;hybrid&quot;)
plot(cluster_DBSCAN, cluster, main=&quot;Clustering using DBSCAN algorithm (eps=3, MinPts=3)&quot;)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-41" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Summary</h1>

<p><space></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-42" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>A root node that has no incoming edges and zero or more outgoing edges.</li>
<li>Internal nodes, each of which has exactly one incoming edge and two or more outgoing edges</li>
<li><p>Leaf or terminal nodes, each of which has exactly one incoming edge and no outgoing edges. </p></li>
<li><p>The non- terminal nodes, which include the root and other internal nodes, contain attribute test conditions to separate records that have different characteristics</p></li>
<li><p>trees work best with categorical values</p></li>
<li><p>descriptions are disjoint</p></li>
<li><p>trees are robust to data errors</p></li>
<li><p>training data is missing values</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-43" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Algorithm(s)</h1>

<p><space></p>

<p>ID3
C4.5
C5.0</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-44" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy calculation</h1>

<p><space></p>

<ul>
<li>The entropy of a sample of data indicates how mixed the class values are; the minimum value of 0 indicates that the sample is completely homogenous, while 1 indicates the maximum amount of disorder.

<ul>
<li>draw curve for entropy</li>
</ul></li>
<li>InfoGain = Entropy (pre split) - Entropy (post split)

<ul>
<li>Entropy is weighted by the Entropy of each feature split

<ul>
<li>avoid pre-pruning because its impossible to know if the tree will miss subtle but important patterns in the data (if you prune too early)</li>
</ul></li>
</ul></li>
<li><p>hard to know optimal length of tree without growing it there first</p></li>
<li><p>Entropy = expected amount of information contained in a random variable -&gt; information is synonymous with &quot;bits&quot; which is why is log, base 2 </p>

<ul>
<li>the more a feature splits the data in obvious ways, the less informative it is for us, entropy is lower</li>
<li>the more the feature splits the data, the higher the entropy and hence information gained by splitting at that feature</li>
<li>Entropy is minimized when one of the events has a P(X)=1</li>
<li>Entropy is maximized when each event has a P(X)=1/n of happening</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-45" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy calculation</h1>

<p><space></p>

<p>entropy_function &lt;- function(p)
  {
    if (min(p) &lt; 0 || sum(p) &lt;= 0)
      return(NA)
    p.norm &lt;- p[p&gt;0]/sum(p)
    -sum(log2(p.norm)*p.norm)
  }</p>

<p>entropy_function(c(0.99,1))</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-46" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<p>voting_data &lt;- read.csv(&#39;<a href="http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data&#x27;">http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data&#39;</a>)
str(voting_data)
names(voting_data) &lt;- c(&#39;party&#39;,&#39;handicapped-infants&#39;,&#39;water-project-cost-sharing&#39;,
                        &#39;adoption-of-the-budget-resolution&#39;,&#39;physician-fee-freeze&#39;,
                        &#39;el-salvador-aid&#39;,&#39;religious-groups-in-schools&#39;,
                        &#39;anti-satellite-test-ban&#39;,&#39;aid-to-nicaraguan-contras&#39;,
                        &#39;mx-missile&#39;,&#39;immigration&#39;,&#39;synfuels-corporation-cutback&#39;,
                        &#39;education-spending&#39;,&#39;superfund-right-to-sue&#39;,&#39;crime&#39;,
                        &#39;duty-free-exports&#39;,&#39;export-administration-act-south-africa&#39;)</p>

<p>prop.table(table(voting_data[,1]))
n &lt;- nrow(voting_data)
train_ind &lt;- sample(n,2/3*n)
voting_train &lt;- voting_data[train_ind,]
voting_test &lt;- voting_data[-train_ind,]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-47" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<p>install.packages(&quot;party&quot;)
library(C50)
library(party)
library(gmodels)
head(voting_train)</p>

<h1>plot tree using party package</h1>

<p>tree_formula &lt;- with(voting_train,voting_train$party ~ .)
p_tree &lt;- ctree(tree_formula,data=voting_train)
plot(p_tree,
     inner_panel=node_inner(p_tree,pval = FALSE,id = TRUE),
     terminal_panel=node_terminal(p_tree, digits = 1, id = TRUE))</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-48" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<h1>make tree using C5.0</h1>

<p>tree_model &lt;- C5.0(voting_train[,-1],voting_train[,1], trials=1)
tree_predict &lt;- predict(tree_model, newdata=voting_test[,-1])
conf &lt;- CrossTable(voting_test[,1], tree_predict, prop.chisq = FALSE,
                   prop.c = FALSE, prop.r = FALSE,
                   dnn = c(&quot;actual class&quot;, &quot;predicted class&quot;))</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-49" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<h1>most important variables</h1>

<p>C5imp(tree_model)</p>

<h1>in-sample error rate</h1>

<p>summary(tree_model)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-50" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example - Boosting</h1>

<p><space></p>

<h1>boosting is rooted in the notion that by combining a number of weak performing learners,</h1>

<h1>you can create a team that is much stronger than any one of the learners alone.</h1>

<h1>this is where C5.0 improves on C4.5</h1>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-51" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example - Boosting</h1>

<p><space></p>

<p>boosted_tree_model &lt;- C5.0(voting_train[,-1],voting_train[,1], trials=25)
boosted_tennis_predict &lt;- predict(boosted_tree_model,voting_test[,-1])</p>

<p>boosted_conf &lt;- CrossTable(voting_test[,1], boosted_tennis_predict, prop.chisq = FALSE,
                           prop.c = FALSE, prop.r = FALSE, 
                           dnn = c(&quot;actual class&quot;, &quot;predicted class&quot;))</p>

<h1>in-sample error rate</h1>

<p>summary(boosted_tree_model)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-52" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example - Error Cost</h1>

<p><space></p>

<h1>still getting too many false positives (predict republican but actually democrat)</h1>

<h1>introduce higher cost to getting this wrong</h1>

<p>error_cost &lt;- matrix(c(0,1,2,0),nrow=2)
cost_model &lt;- C5.0(voting_train[,-1],voting_train[,1], trials=1, costs = error_cost)
cost_predict &lt;- predict(cost_model, newdata=voting_test[,-1])
conf &lt;- CrossTable(voting_test[,1], cost_predict, prop.chisq = FALSE,
                   prop.c = FALSE, prop.r = FALSE,
                   dnn = c(&quot;actual class&quot;, &quot;predicted class&quot;))</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-53" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example - Error Cost</h1>

<p><space></p>

<p>tris &lt;- seq(1,50,by=2)
boost_acc &lt;- NULL
for (i in tris){<br>
  temp &lt;- C5.0(voting_train[,-1],voting_train[,1], trials=i, costs = error_cost)
  temp_pred &lt;- predict(temp,voting_test[,-1])
  boost_acc &lt;- append(boost_acc,sum(diag(table(temp_pred,voting_test[,1]))))
}</p>

<p>plot(boost_acc,type=&#39;l&#39;)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-54" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Pros and Cons</h1>

<p><space></p>

<ul>
<li>trees are non-parametric, rule based classification or regression method</li>
<li>simple to understand and interpret</li>
<li>little data preparation</li>
<li>easy to overfit (need to prune to avoid that, or have max tree depth)</li>
<li>the global optimum is known as NP-complete unless greedy search is employed, which is then only locally optimum. Can mitigate this with an ensemble of trees</li>
<li>difficult concepts that are not easily expressed by trees (XOR) are hard to learn</li>
<li>for class imbalance, trees can be biased - should balance dataset before fitting</li>
<li>trees tend to overfit, so use PCA beforehand</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-55" style="background:;">
  <hgroup>
    <h2>Resources</h2>
  </hgroup>
  <article>
    <p><space></p>

<ul>
<li><a href="http://www.packtpub.com/machine-learning-with-r/book">Machine Learning with R</a></li>
<li><a href="http://shop.oreilly.com/product/0636920018483.do">Machine Learning for Hackers</a></li>
<li><a href="http://web.stanford.edu/%7Ehastie/local.ftp/Springer/OLD/ESLII_print4.pdf">Elements of Statistical Learning</a></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-56" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    
  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>

  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
<!-- Grab CDN jQuery, fall back to local if offline -->
<script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
<script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery-1.7.min.js"><\/script>')</script>
<!-- Load Javascripts for Widgets -->
<!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->
<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
<script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- DONE LOADING HIGHLIGHTER JS FILES -->
</html>