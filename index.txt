<!DOCTYPE html>
<html>
<head>
<style type="text/css">
.knitr.inline {
  background-color: #f7f7f7;
  border:solid 1px #B0B0B0;
}
.error {
	font-weight: bold;
	color: #FF0000;
},
.warning {
	font-weight: bold;
}
.message {
	font-style: italic;
}
.source, .output, .warning, .error, .message {
	padding: 0em 1em;
  border:solid 1px #F7F7F7;
}
.source {
  background-color: #f5f5f5;
}
.rimage.left {
  text-align: left;
}
.rimage.right {
  text-align: right;
}
.rimage.center {
  text-align: center;
}
.hl.num {
  color: #AF0F91;
}
.hl.str {
  color: #317ECC;
}
.hl.com {
  color: #AD95AF;
  font-style: italic;
}
.hl.opt {
  color: #000000;
}
.hl.std {
  color: #585858;
}
.hl.kwa {
  color: #295F94;
  font-weight: bold;
}
.hl.kwb {
  color: #B05A65;
}
.hl.kwc {
  color: #55aa55;
}
.hl.kwd {
  color: #BC5A65;
  font-weight: bold;
}
</style>
  <title>Machine Learning with R - Part II</title>
  <meta charset="utf-8">
  <meta name="description" content="Machine Learning with R - Part II">
  <meta name="author" content="Ilan Man">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
    
</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
    <!-- END LOGO SLIDE -->
    

    <!-- TITLE SLIDE -->
    <!-- Should I move this to a Local Layout File? -->
    <slide class="title-slide segue nobackground">
      <hgroup class="auto-fadein">
        <h1>Machine Learning with R - Part II</h1>
        <h2></h2>
        <p>Ilan Man<br/>Strategy Operations  @ Squarespace</p>
      </hgroup>
          </slide>

    <!-- SLIDES -->
      <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Agenda</h2>
  </hgroup>
  <article>
    <p><space></p>

<ol>
<li>Logistic Regression</li>
<li>Principle Component Analysis</li>
<li>Clustering</li>
<li>Trees</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Objectives</h2>
  </hgroup>
  <article>
    <p><space></p>

<ol>
<li>Understand some popular algorithms and techniques</li>
<li>Learn how to tune parameters</li>
<li>Practice R</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p><img src="figure/log_bad_fit.png" alt="plot of chunk log_bad_fit"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p><img src="figure/log_bad_fit2.png" alt="plot of chunk log_bad_fit2"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p><img src="figure/log_bad_fit3.png" alt="plot of chunk log_bad_fit3"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p><img src="figure/log_motivation.png" alt="plot of chunk log_motivation"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p><img src="figure/log_motivation2.png" alt="plot of chunk log_motivation2"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p><img src="figure/log_motivation3.png" alt="plot of chunk log_motivation3"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>Binary response variable (Y = 1 or Y = 0) association to a set of explanatory variables</li>
<li>Like Linear Regression with a categorical outcome</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>Binary response variable (Y = 1 or Y = 0) association to a set of explanatory variables</li>
<li>Like Linear Regression with a categorical outcome</li>
<li>\(\hat{y} = \beta_{0} + \beta_{1}x_{1} + ... + \beta_{n}x_{n}\)</li>
<li>becomes<br></li>
<li>\(\log{\frac{P(Y=1)}{1 - P(Y=1)}} = \beta_{0} + \beta_{1}x_{1} + ... + \beta_{n}x_{n}\)</li>
<li>Can be extended to multiple and/or ordered categories</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>Family of GLMs</li>
<li>Random component

<ul>
<li>Noise or Errors</li>
</ul></li>
<li>Systematic Component

<ul>
<li>Linear combination in \(X_{i}\)</li>
</ul></li>
<li>Link Function

<ul>
<li>Connects Random and Systematic components</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>Data is I.I.D.

<ul>
<li>\(Y\)&#39;s assume to come from family of exponential distributions</li>
</ul></li>
<li>Uses MLE to determine parameters - Not OLS

<ul>
<li>MLE satisfies lots of nice properties</li>
<li>Does not require transformation of \(Y\)&#39;s to be Normal</li>
<li>Does not require constant variance</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>Type of regression to predict the probability of being in a class

<ul>
<li>Output is \(P(Y=1 | X)\)</li>
<li>Typical threshold is 0.5</li>
</ul></li>
<li>Assumes error terms are Binomially distributed

<ul>
<li>Generates 1&#39;s and 0&#39;s as the error term</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>Type of regression to predict the probability of being in a class

<ul>
<li>Output is \(P(Y=1 | X)\)</li>
<li>Typical threshold is 0.5</li>
</ul></li>
<li>Assumes error terms are Binomially distributed

<ul>
<li>Generates 1&#39;s and 0&#39;s as the error term</li>
</ul></li>
<li>Sigmoid (logistic) function: \(g(z) = \frac{1}{1+e^{-z}}\)

<ul>
<li>Bounded by 0 and 1</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="figure/log_curve.png" alt="plot of chunk log_curve"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>The hypothesis function, \(h_{\theta}(x)\), is \(P(Y=1|X)\)</li>
<li>Linear regression: \(h_{\theta}(x) = \theta x^{T}\)</li>
<li>Logistic regression: \(h_{\theta}(x) = g(\theta x^{T})\) 
<br>
where \(g(z) = \frac{1}{1+e^{-z}}\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Notation</h1>

<p><space></p>

<ul>
<li>Re-arranging \(Y = \frac{1}{1+e^{-\theta x^{T}}}\) yields
<br>
\(\log{\frac{Y}{1 - Y}} = \theta x^{T}\)</li>
<li><strong>Log odds</strong> are linear in \(X\)</li>
<li>This is called the logit of \(Y\)

<ul>
<li>Links the odds of \(Y\) (a probability) to a linear regression in \(X\)</li>
<li>Logit ranges from -ve infite to +ve infinite</li>
<li>When \(x_{1}\) increases by 1 unit, \(P(Y=1)\) increases by \(e^{\theta_{1}}\)</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>So \(h_{\theta}(x) = \frac{1}{1+e^{-\theta x^{T}}}\)</li>
<li>Cost function?</li>
<li>Why can&#39;t we use the same cost function as for the linear hypothesis?</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>So \(h_{\theta}(x) = \frac{1}{1+e^{-\theta x^{T}}}\)</li>
<li>Cost function?</li>
<li>Why can&#39;t we use the same cost function as for the linear hypothesis?

<ul>
<li>Logistic residuals are Binomially distributed</li>
<li>Regression function is not linear in \(X\)</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>\(Y\) can be 1 or 0 (binary case)</li>
<li>\(Y | X\) ~ Bernoulli</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>\(Y\) can be 1 or 0 (binary case)</li>
<li>\(Y | X\) ~ Bernoulli</li>
<li>\(P(Y|X) = p\), when \(Y\) = 1</li>
<li>\(P(Y|X) = 1-p\), when \(Y\) = 0</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>\(Y\) can be 1 or 0 (binary case)</li>
<li>\(Y | X\) ~ Bernoulli</li>
<li>\(P(Y|X) = p\), when \(Y\) = 1</li>
<li>\(P(Y|X) = 1-p\), when \(Y\) = 0</li>
<li>\(P(Y = y_{i}|X) = p^{y_{i}}(1-p)^{1-y_{i}}\)</li>
<li>Taking the log of both sides...</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-23" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<p>\(cost(h_{\theta}(x), y) = -y \log(h_{\theta}(x)) + (1-y) \log(1-h_{\theta}(x))\)<br></p>

<p><img src="figure/cost_curves1.png" alt="plot of chunk cost_curves"> <img src="figure/cost_curves2.png" alt="plot of chunk cost_curves"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-24" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<p>\(cost(h_{\theta}(x), y) = -y \log(h_{\theta}(x)) + (1-y) \log(1-h_{\theta}(x))\)<br></p>

<ul>
<li>Logistic regression cost function is then<br>
\(cost(h_{\theta}(x), y)  = \frac{1}{m} \sum_{i=1}^{m} -y \log(h_{\theta}(x)) + (1-y) \log(1-h_{\theta}(x))\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-25" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<p>\(cost(h_{\theta}(x), y) = -y \log(h_{\theta}(x)) + (1-y) \log(1-h_{\theta}(x))\)<br></p>

<ul>
<li>Logistic regression cost function is then<br>
\(cost(h_{\theta}(x), y)  = \frac{1}{m} \sum_{i=1}^{m} -y \log(h_{\theta}(x_{i})) + (1-y) \log(1-h_{\theta}(x_{i}))\)</li>
<li>Minimize the cost</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-26" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Find parameters</h1>

<p><space></p>

<ul>
<li>Cannot solve analytically</li>
<li>Use approximation methods

<ul>
<li>(Stochastic) Gradient Descent</li>
<li>Conjugate Descent</li>
<li>Newton-Raphson Method</li>
<li>BFGS</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-27" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<ul>
<li>Efficient</li>
<li>Easier to calculate that gradient descent

<ul>
<li>Except for first and second derivatives</li>
</ul></li>
<li>Converges on <em>global</em> minimum</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-28" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<ul>
<li>Assume \(f'(x_{0})\) is close to zero and \(f''(x_{0})\) is positive</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-29" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<ul>
<li>Assume \(f'(x_{0})\) is close to zero and \(f''(x_{0})\) is positive</li>
<li>Re-write \(f(x)\) as its Taylor expansion:<br>
\(f(x) = f(x_{0}) + (x-x_{0})f'(x_{0}) + \frac{1}{2}(x-x_{0})^{2}f''(x_{0})\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-30" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<ul>
<li>Assume \(f'(x_{0})\) is close to zero and \(f''(x_{0})\) is positive</li>
<li>Re-write \(f(x)\) as its Taylor expansion:<br>
\(f(x) = f(x_{0}) + (x-x_{0})f'(x_{0}) + \frac{1}{2}(x-x_{0})^{2}f''(x_{0})\)</li>
<li>Take the derivative w.r.t \(x\) and set = 0<br>
\(0 = f'(x_{0}) + \frac{1}{2}f''(x_{0})2(x_{1} − x_{0})\)<br>
\(x_{1} = x_{0} − \frac{f'(x_{0})}{f￼''(x_{0})}\)

<ul>
<li>\(x_{1}\) is a better approximation for the minimum than \(x_{0}\)</li>
<li>and so on...</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-31" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<p>\(f(x) = x^{4} - 3\log(x)\)</p>

<p><img src="figure/newton_curve.png" alt="plot of chunk newton_curve"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-32" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<pre><code class="r">fn &lt;- function(x) x^4 - 3*log(x)
dfn &lt;- function(x) 4*x^3 - 3/x
d2fn &lt;- function(x) 12*x^2 + 3/x^2 

newton &lt;- function(num.its, dfn, d2fn){
  theta &lt;- rep(0,num.its)
  theta[1] &lt;- round(runif(1,0,100),0)

  for (i in 2:num.its) {
    h &lt;- - dfn(theta[i-1]) / d2fn(theta[i-1])
    theta[i] &lt;- theta[i-1] + h 
  }

  out &lt;- cbind(1:num.its,theta)
  dimnames(out)[[2]] &lt;- c(&quot;iteration&quot;,&quot;estimate&quot;)
  return(out)
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-33" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<pre><code>     iteration estimate
[1,]         1    97.00
[2,]         2    64.67
[3,]         3    43.11
[4,]         4    28.74
[5,]         5    19.16
</code></pre>

<pre><code>      iteration estimate
[16,]        16   0.9306
[17,]        17   0.9306
[18,]        18   0.9306
[19,]        19   0.9306
[20,]        20   0.9306
</code></pre>

<pre><code>[1] 0.9658
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-34" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson Method</h1>

<p><space></p>

<pre><code class="r">optimize(fn,c(-100,100))  ## built-in R optimization function
</code></pre>

<pre><code>$minimum
[1] 0.9306

$objective
[1] 0.9658
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-35" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Newton-Raphson</h1>

<p><space></p>

<ul>
<li>Minimization algorithm</li>
<li>Approximation, non-closed form solution</li>
<li>Built-in to many programs</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-36" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article>
    <h1>Summary</h1>

<p><space></p>

<ul>
<li>Very popular classification algorithm</li>
<li>Part of family of GLMs</li>
<li>Based on Binomial error terms, i.e. 1&#39;s and 0&#39;s</li>
<li>Usually requires large sample size</li>
<li>Assumes linearity between logit function and independent variables</li>
<li>Does not work out of the box with correlated features...</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-37" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>Unsupervised learning</li>
<li>Used widely in modern data analysis</li>
<li>Compute the most meaningful way to re-express noisy data, revealing the hidden structure</li>
<li>Commonly used to supplement supervised learning algorithms</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-38" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/original_data.png" alt="original_data"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-39" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/calc_centroid.png" alt="calc_centroid"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-40" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/sub_mean.png" alt="sub_mean"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-41" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/max_var_dir.png" alt="max_var_dir"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-42" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/second_PC.png" alt="second_PC"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-43" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/rotated_grid.png" alt="rotated_grid"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-44" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/rotated_PCs.png" alt="rotated_PCs"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-45" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/new_axes.png" alt="new_axes"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-46" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<p><img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/final_PC.png" alt="final_PC"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-47" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>Assumes linearity</li>
<li>\(\bf{PX}=\bf{Y}\)

<ul>
<li>\(\bf{X}\) is original dataset, \(\bf{P}\) is a transformation of \(\bf{X}\) into \(\bf{Y}\)</li>
</ul></li>
<li>How to choose \(\bf{P}\)?<br>
1) Reduce noise<br>
2) Maximize variance</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-48" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li><p>Covariance matrix<br>
\(\bf{C} = \bf{XX}^{T}\)</p></li>
<li><p>Restated goals are</p>

<ul>
<li>Minimize covariance and maximize variance</li>
<li>Optimal \(\bf{C}\) is a diagonal matrix, off diagonals are = 0</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-49" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Concepts</h1>

<p><space></p>

<ul>
<li>Assumes linear relationship between \(\bf{X}\) and \(\bf{Y}\) (non-linear is a kernel PCA)</li>
<li>Largest variance indicates most signal</li>
<li>Orthogonal components - makes the linear algebra easier</li>
<li>Assumes data is normally distributed, otherwise PCA might not diagonalize matrix

<ul>
<li>Can use ICA...</li>
<li>But most data is normal and PCA is robust to slight deviance from normality</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-50" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<p>\(\bf{A}x = \lambda x\)</p>

<ul>
<li>\(\lambda\) is an eigenvalue of \(\bf{A}\) and \(x\) is an eigenvector of \(\bf{A}\)<br>
\(\bf{A}x - \lambda Ix = 0\)<br>
\((\bf{A} - \lambda I)x = 0\)<br>
\(\det(\bf{A} - \lambda I)\) = 0 &nbps; &lt;- roots of this yield eigenvalues of \(\bf{A}\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-51" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<p>\[A = \begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix}, I= \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix}, X = \begin{bmatrix} x_{1}\\ x_{2} \end{bmatrix}\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-52" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<p>\[A = \begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix}, I= \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix}, X = \begin{bmatrix} x_{1}\\ x_{2} \end{bmatrix}\]
\[\begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix}X = \lambda X\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-53" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<p>\[A = \begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix}, I= \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix}, X = \begin{bmatrix} x_{1}\\ x_{2} \end{bmatrix}\]
\[\begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix}X = \lambda X\]
\[\begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix}X - \lambda X = 0\]
\[(\begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix} - \lambda I)X = 0\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-54" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<p>\[\begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix}X = \lambda X\]
\[\begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix}X - \lambda X = 0\]
\[(\begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix} - \lambda I)X = 0\]
\[\left | \begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix} - \lambda I \right |= 0\]
\[\left|\begin{bmatrix} 5 & 2\\ 2 & 5 \end{bmatrix} - \lambda \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix} \right| = 0\]
\[\left|\begin{bmatrix} 5-\lambda & 2\\ 2 & 5-\lambda \end{bmatrix}\right| = 0\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-55" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<p>\((5-\lambda)\times(5-\lambda) - 4 = 0\)
<br>
\(\lambda^{2} - 10\lambda + 21 = 0\)
<br>
\(\lambda = ?\)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-56" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<pre><code class="r">A = matrix(c(5,2,2,5),nrow=2)
roots &lt;- Re(polyroot(c(21,-10,1)))
roots
</code></pre>

<pre><code>## [1] 3 7
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-57" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<ul>
<li>when \(\lambda = 3\)<br>
\(Ax = 3x\)<br></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-58" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<ul>
<li>when \(\lambda = 3\)<br>
\(Ax = 3x\)<br>
\(5x_{1} + 2x_{2} = 3x_{1}\)<br>
\(2x_{1} + 5x_{2} = 3x_{2}\)<br></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-59" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<ul>
<li>when \(\lambda = 3\)<br>
\(Ax = 3x\)<br>
\(5x_{1} + 2x_{2} = 3x_{1}\)<br>
\(2x_{1} + 5x_{2} = 3x_{2}\)<br>
\(x_{1} = -x_{2}\)<br></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-60" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<ul>
<li>when \(\lambda = 3\)<br>
\(Ax = 3x\)<br>
\(5x_{1} + 2x_{2} = 3x_{1}\)<br>
\(2x_{1} + 5x_{2} = 3x_{2}\)<br>
\(x_{1} = -x_{2}\)<br></li>
</ul>

<p>\[Eigenvector = \begin{bmatrix} 1\\ -1 \end{bmatrix}\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-61" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<ul>
<li>when \(\lambda = 7\)<br>
\(Ax = 7x\)<br>
\(5x_{1} + 2x_{2} = 7x_{1}\)<br>
\(2x_{2} + 5x_{2} = 7x_{2}\)<br>
\(x_{1} = x_{2}\)<br></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-62" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<ul>
<li>when \(\lambda = 7\)<br>
\(Ax = 7x\)<br>
\(5x_{1} + 2x_{2} = 7x_{1}\)<br>
\(2x_{2} + 5x_{2} = 7x_{2}\)<br>
\(x_{1} = x_{2}\)<br></li>
</ul>

<p>\[Eigenvector = \begin{bmatrix} 1\\ 1 \end{bmatrix}\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-63" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<p>\(Ax = \lambda x\)</p>

<pre><code class="r">A %*% c(1,-1) == 3 * as.matrix(c(1,-1))
A %*% c(1,1) == 7 * as.matrix(c(1,1))
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-64" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<p>\(Ax = \lambda x\)</p>

<pre><code class="r">A %*% c(1,-1) == 3 * as.matrix(c(1,-1))
</code></pre>

<pre><code>     [,1]
[1,] TRUE
[2,] TRUE
</code></pre>

<pre><code class="r">A %*% c(1,1) == 7 * as.matrix(c(1,1))
</code></pre>

<pre><code>     [,1]
[1,] TRUE
[2,] TRUE
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-65" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Eigenwhat?</h1>

<p><space></p>

<pre><code class="r">m &lt;- matrix(c(1,-1,1,1),ncol=2)   ## two eigenvectors
m &lt;- m/sqrt(norm(m))  ## normalize
as.matrix(m%*%diag(roots)%*%t(m))
</code></pre>

<pre><code>##      [,1] [,2]
## [1,]    5    2
## [2,]    2    5
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-66" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p>\(\bf{PX} = \bf{Y}\)<br>
\(\bf{C_{Y}} = \frac{1}{(n-1)}\bf{YY^{T}}\)<br></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-67" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p>\(\bf{PX} = \bf{Y}\)<br>
\(\bf{C_{Y}} = \frac{1}{(n-1)}\bf{YY^{T}}\)<br>
\(=\bf{PX(PX)^{T}}\)<br>
\(=\bf{P(XX^{T})P^{T}}\)<br></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-68" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<p>\(\bf{PX} = \bf{Y}\)<br>
\(\bf{C_{Y}} = \frac{1}{(n-1)}\bf{YY^{T}}\)<br>
\(=\bf{PX(PX)^{T}}\)<br>
\(=\bf{P(XX^{T})P^{T}}\)<br>
\(=\bf{PAP^{T}}\)<br></p>

<ul>
<li>\(\bf{P}\) is a matrix with columns that are eigenvectors</li>
<li>\(\bf{A}\) is a diagonalized matrix of eigenvalues and is symmetric<br>
\(\bf{A} = \bf{EDE^{T}}\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-69" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>Each row of \(\bf{P}\) should be an eigenvector of \(\bf{A}\)<br>
\(\bf{P} = \bf{E^{T}}\)</li>
<li>Note that \(\bf{P^{T}} = \bf{P^{-1}}\) (linear algebra)<br></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-70" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>Each row of \(\bf{P}\) should be an eigenvector of \(\bf{A}\)<br>
\(\bf{P} = \bf{E^{T}}\)</li>
<li>Note that \(\bf{P^{T}} = \bf{P^{-1}}\) (linear algebra)<br>
\(\bf{A} = \bf{P^{T}DP}\)<br>
\(\bf{C_{Y}} = \bf{PAP^{T}}\)<br>
\(\bf{C_{Y}} = \bf{PP^{T}DPP^{T}} = \frac{1}{n-1}\bf{D}\)</li>
<li>\(\bf{D}\) is a diagonal matrix, depending on how we choose \(\bf{P}\)</li>
<li>Therefore \(\bf{C_{Y}}\) is diagonalized</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-71" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">data &lt;- read.csv(&#39;tennis_data_2013.csv&#39;)
</code></pre>

<pre><code>## Warning: cannot open file &#39;tennis_data_2013.csv&#39;: No such file or
## directory
</code></pre>

<pre><code>## Error: cannot open the connection
</code></pre>

<pre><code class="r">data$Player1 &lt;- as.character(data$Player1)
</code></pre>

<pre><code>## Error: replacement has 0 rows, data has 6497
</code></pre>

<pre><code class="r">data$Player2 &lt;- as.character(data$Player2)
</code></pre>

<pre><code>## Error: replacement has 0 rows, data has 6497
</code></pre>

<pre><code class="r">tennis &lt;- data
m &lt;- length(data)

for (i in 10:m){
  tennis[,i] &lt;- ifelse(is.na(data[,i]),0,data[,i])
}

str(tennis)
</code></pre>

<pre><code>## &#39;data.frame&#39;:    6497 obs. of  13 variables:
##  $ type                : Factor w/ 2 levels &quot;red&quot;,&quot;white&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ fixed.acidity       : num  7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 7.5 ...
##  $ volatile.acidity    : num  0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.5 ...
##  $ citric.acid         : num  0 0 0.04 0.56 0 0 0.06 0 0.02 0.36 ...
##  $ residual.sugar      : num  1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 6.1 ...
##  $ chlorides           : num  0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.071 ...
##  $ free.sulfur.dioxide : num  11 25 15 17 11 13 15 15 9 17 ...
##  $ total.sulfur.dioxide: num  34 67 54 60 34 40 59 21 18 102 ...
##  $ density             : num  0.998 0.997 0.997 0.998 0.998 ...
##  $ pH                  : num  3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ...
##  $ sulphates           : num  0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ...
##  $ alcohol             : num  9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ...
##  $ quality             : int  5 5 5 6 5 5 5 7 7 5 ...
</code></pre>

<pre><code class="r">features &lt;- tennis[,10:m]

head(features)
</code></pre>

<pre><code>##     pH sulphates alcohol quality
## 1 3.51      0.56     9.4       5
## 2 3.20      0.68     9.8       5
## 3 3.26      0.65     9.8       5
## 4 3.16      0.58     9.8       6
## 5 3.51      0.56     9.4       5
## 6 3.51      0.56     9.4       5
</code></pre>

<pre><code class="r">str(features)
</code></pre>

<pre><code>## &#39;data.frame&#39;:    6497 obs. of  4 variables:
##  $ pH       : num  3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ...
##  $ sulphates: num  0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ...
##  $ alcohol  : num  9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ...
##  $ quality  : int  5 5 5 6 5 5 5 7 7 5 ...
</code></pre>

<pre><code class="r">dim(features)
</code></pre>

<pre><code>## [1] 6497    4
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-72" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">scaled_features &lt;- as.matrix(scale(features))
Cx &lt;- cov(scaled_features)
eigenvalues &lt;- eigen(Cx)$values
eigenvectors &lt;- eigen(Cx)$vectors
PC &lt;- scaled_features %*% eigenvectors
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-73" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">Cy &lt;- cov(PC)
sum(round(diag(Cy) - eigenvalues,5))
</code></pre>

<pre><code>## [1] 0
</code></pre>

<pre><code class="r">sum(round(Cy[upper.tri(Cy)],5)) ## off diagonals are 0 since PC&#39;s are orthogonal
</code></pre>

<pre><code>## [1] 0
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-74" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<p><img src="figure/var_expl_plot.png" alt="plot of chunk var_expl_plot"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-75" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">pca.df &lt;- prcomp(scaled_features)
eigenvalues == (pca.df$sdev)^2
</code></pre>

<pre><code>## [1] FALSE FALSE FALSE FALSE
</code></pre>

<pre><code class="r">eigenvectors[,1] == pca.df$rotation[,1]
</code></pre>

<pre><code>##        pH sulphates   alcohol   quality 
##     FALSE     FALSE     FALSE     FALSE
</code></pre>

<pre><code class="r">sum((eigenvectors[,1])^2)
</code></pre>

<pre><code>## [1] 1
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-76" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code>## Error: object &#39;gender&#39; not found
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-77" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<ul>
<li>Classify based on PC1?</li>
</ul>

<pre><code class="r">gen &lt;- ifelse(pca.df$x[,1] &gt; abs(mean(pca.df$x[,1]))*2,&quot;F&quot;,&quot;M&quot;)
sum(diag(table(gen,as.character(data$Gender))))/rows
</code></pre>

<pre><code>## Error: all arguments must have the same length
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-78" style="background:;">
  <hgroup>
    <h2>Principle Component Analysis</h2>
  </hgroup>
  <article>
    <h1>Summary</h1>

<p><space></p>

<ul>
<li>Very popular dimensionality reduction technique</li>
<li>Intuitive</li>
<li>Cannot reverse engineer dataset easily</li>
<li>Sparse PCA emphasizes important features</li>
<li>Non-linear structure is difficult to model with PCA</li>
<li>Extensions (ICA, kernel PCA) developed to generalize</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-79" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>Separate data into meaningful or useful groups

<ul>
<li>Capture natural structure of the data</li>
<li>Starting point for further analysis</li>
</ul></li>
<li>Cluster for utility

<ul>
<li>Summarizing data for less expensive computation</li>
<li>Data compression</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-80" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Types of Clusters</h1>

<p><space></p>

<ul>
<li>Data that looks similar</li>
<li>Prototype based</li>
<li>Density based</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-81" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Typical clustering problem</h1>

<p><space></p>

<p><img src="figure/cluster_plot_example.png" alt="plot of chunk cluster_plot_example"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-82" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Density based cluster</h1>

<p><space></p>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/0/05/DBSCAN-density-data.svg" density_based /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-83" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans algorithm</h1>

<p><space></p>

<ul>
<li>Select K points as initial centroids </li>
<li>Do

<ul>
<li>Form K clusters by assigning each point to its closest centroid</li>
<li>Recompute the centroid of each cluster </li>
</ul></li>
<li>Until centroids do not change, or change very minimally, i.e. &lt;1%</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-84" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans algorithm</h1>

<p><space></p>

<ul>
<li>Use similarity measures (Euclidean or cosine) depending on the data</li>
<li>Minimize the squared distance of each point to closest centroid
\(SSE(k) = \sum_{i=1}^{m}\sum_{j=1}^{n} (x_{ij} - \bar{x}_{kj})\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-85" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans - notes</h1>

<p><space></p>

<ul>
<li>Choose initial K randomly 

<ul>
<li>can lead to poor centroids - local minimuum</li>
<li>Run kmeans multiple times</li>
</ul></li>
<li>Reduce the total SSE by increasing K</li>
<li>Increase the cluster with largest SSE</li>
<li>Decrease K and minimize SSE</li>
<li>Split up a cluster into other clusters

<ul>
<li>The centroid that is split will increase total SSE the least</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-86" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans</h1>

<p><space></p>

<ul>
<li>Bisecting K means

<ul>
<li>Split points into 2 clusters</li>
<li>Take cluster with largest SSE - split that into two clusters</li>
<li>Rerun bisecting K mean on resulting clusters</li>
<li>Stop when you have K clusters</li>
</ul></li>
<li>Less susceptible to initialization problems</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-87" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmean fails</h1>

<p><space></p>

<p><img src="C:/Users/Ilan%20Man/Desktop/Personal/RPres_ML_2/figure/different_density.png" alt="different_density"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-88" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmean fails</h1>

<p><space></p>

<p><img src="C:/Users/Ilan%20Man/Desktop/Personal/RPres_ML_2/figure/different_size_clusters.png" alt="different_size_clusters"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-89" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmean fails</h1>

<p><space></p>

<p><img src="C:/Users/Ilan%20Man/Desktop/Personal/RPres_ML_2/figure/non-globular.png" alt="non-globular"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-90" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans</h1>

<p><space></p>

<pre><code class="r">wine &lt;- read.csv(&#39;http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&#39;)
names(wine) &lt;- c(&quot;class&quot;,&#39;Alcohol&#39;,&#39;Malic&#39;,&#39;Ash&#39;,&#39;Alcalinity&#39;,&#39;Magnesium&#39;,&#39;Total_phenols&#39;,
                 &#39;Flavanoids&#39;,&#39;NFphenols&#39;,&#39;Proanthocyanins&#39;,&#39;Color&#39;,&#39;Hue&#39;,&#39;Diluted&#39;,&#39;Proline&#39;)
str(wine)
</code></pre>

<pre><code>## &#39;data.frame&#39;:    177 obs. of  14 variables:
##  $ class          : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ Alcohol        : num  13.2 13.2 14.4 13.2 14.2 ...
##  $ Malic          : num  1.78 2.36 1.95 2.59 1.76 1.87 2.15 1.64 1.35 2.16 ...
##  $ Ash            : num  2.14 2.67 2.5 2.87 2.45 2.45 2.61 2.17 2.27 2.3 ...
##  $ Alcalinity     : num  11.2 18.6 16.8 21 15.2 14.6 17.6 14 16 18 ...
##  $ Magnesium      : int  100 101 113 118 112 96 121 97 98 105 ...
##  $ Total_phenols  : num  2.65 2.8 3.85 2.8 3.27 2.5 2.6 2.8 2.98 2.95 ...
##  $ Flavanoids     : num  2.76 3.24 3.49 2.69 3.39 2.52 2.51 2.98 3.15 3.32 ...
##  $ NFphenols      : num  0.26 0.3 0.24 0.39 0.34 0.3 0.31 0.29 0.22 0.22 ...
##  $ Proanthocyanins: num  1.28 2.81 2.18 1.82 1.97 1.98 1.25 1.98 1.85 2.38 ...
##  $ Color          : num  4.38 5.68 7.8 4.32 6.75 5.25 5.05 5.2 7.22 5.75 ...
##  $ Hue            : num  1.05 1.03 0.86 1.04 1.05 1.02 1.06 1.08 1.01 1.25 ...
##  $ Diluted        : num  3.4 3.17 3.45 2.93 2.85 3.58 3.58 2.85 3.55 3.17 ...
##  $ Proline        : int  1050 1185 1480 735 1450 1290 1295 1045 1045 1510 ...
</code></pre>

<ul>
<li>set.seed() to make sure results are reproducible</li>
<li>add nstart to the function call so that it attempts multiple configurations, selecting the best</li>
<li>use a screeplot to select optimal K</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-91" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans</h1>

<p><space></p>

<pre><code class="r">s.wine &lt;- scale(wine[,-1])
best_k &lt;- 0
num_k &lt;- 20
for (i in 1:num_k){
  best_k[i] &lt;- sum(kmeans(s.wine,centers=i)$withinss)
  }

barplot(best_k, xlab = &quot;Number of clusters&quot;,
        names.arg = 1:num_k,
        ylab=&quot;Within groups sum of squares&quot;,
        main=&quot;Scree Plot for Wine dataset&quot;)
</code></pre>

<p><img src="figure/unnamed-chunk-2.png" alt="plot of chunk unnamed-chunk-2"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-92" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Kmeans animation</h1>

<p><space></p>

<p>install.packages(&#39;animation&#39;)
library(animation)</p>

<p>oopt = ani.options(interval = 1)
ani_ex = rbind(matrix(rnorm(100, sd = 0.3), ncol = 2), 
          matrix(rnorm(100, sd = 0.3), 
          ncol = 2))
colnames(ani_ex) = c(&quot;x&quot;, &quot;y&quot;)</p>

<p>kmeans.an = function(
  x = cbind(X1 = runif(50), X2 = runif(50)), centers = 4, hints = c(&#39;Move centers!&#39;, &#39;Find cluster?&#39;),
  pch = 1:5, col = 1:5
) {
  x = as.matrix(x)
  ocluster = sample(centers, nrow(x), replace = TRUE)
  if (length(centers) == 1) centers = x[sample(nrow(x), centers), ] else
    centers = as.matrix(centers)
  numcent = nrow(centers)
  dst = matrix(nrow = nrow(x), ncol = numcent)
  j = 1
  pch = rep(pch, length = numcent)
  col = rep(col, length = numcent)</p>

<p>for (j in 1:ani.options(&#39;nmax&#39;)) {
    dev.hold()
    plot(x, pch = pch[ocluster], col = col[ocluster], panel.first = grid())
    mtext(hints[1], 4)
    points(centers, pch = pch[1:numcent], cex = 3, lwd = 2, col = col[1:numcent])
    ani.pause()
    for (i in 1:numcent) {
      dst[, i] = sqrt(apply((t(t(x) - unlist(centers[i, ])))<sup>2,</sup> 1, sum))
    }
    ncluster = apply(dst, 1, which.min)
    plot(x, type = &#39;n&#39;)
    mtext(hints[2], 4)
    grid()
    ocenters = centers
    for (i in 1:numcent) {
      xx = subset(x, ncluster == i)
      polygon(xx[chull(xx), ], density = 10, col = col[i], lty = 2)
      points(xx, pch = pch[i], col = col[i])
      centers[i, ] = apply(xx, 2, mean)
    }
    points(ocenters, cex = 3, col = col[1:numcent], pch = pch[1:numcent], lwd = 2)
    ani.pause()
    if (all(ncluster == ocluster)) break
    ocluster = ncluster
  }
  invisible(list(cluster = ncluster, centers = centers))
}</p>

<p>kmeans.an(ani_ex, centers = 5, hints = c(&quot;Move centers&quot;,&quot;Cluster found?&quot;))</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-93" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>K-medoid</h1>

<p><space></p>

<ul>
<li>multiple distance metrics</li>
<li>robust medioids</li>
<li>computationally expensive</li>
<li>cluster center is one of the points itself</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-94" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>K-medoid</h1>

<p><space></p>

<ul>
<li>cluster each point based on the closest center</li>
<li>replace each center by the medioid of points in its cluster</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-95" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>K-medoid</h1>

<p><space></p>

<ul>
<li>Selecting the optimal number of clusters</li>
<li>For each point p, first find the average distance between p and all other points in the same cluster, \(A\)</li>
<li>Then find the average distance between p and all points in the nearest cluster, \(B\)</li>
<li>The silhouette coefficient for p is \(\frac{A - B}{\max(A,B)}\)

<ul>
<li>Values close to 1 mean point clearly belongs to that cluster</li>
<li>Values close to 0 mean points might belong in another cluster</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-96" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>K-medoid</h1>

<p><space></p>

<pre><code class="r">library(cluster)

pam.best &lt;- as.numeric()
for (i in 2:20){
  pam.best[i] &lt;- pam(s.wine, k=i)$silinfo$avg.width
}
best_k &lt;- which.max(pam.best)
best_k
</code></pre>

<pre><code>## [1] 3
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-97" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>K-medoid</h1>

<p><space></p>

<pre><code class="r">clusplot(pam(s.wine,best_k), main=&quot;K-medoids with K = 3&quot;,sub=NULL)
</code></pre>

<p><img src="figure/unnamed-chunk-3.png" alt="plot of chunk unnamed-chunk-3"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-98" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>DBSCAN</h1>

<p><space></p>

<ul>
<li>A cluster is a dense region of points separated by low-density regions</li>
<li>Group objects into one cluster if they are connected to one another by densely populated area</li>
</ul>

<h2>- Used when the clusters are irregular or intertwined, and when noise and outliers are present</h2>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-99" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Terminology</h1>

<p><space></p>

<ul>
<li>Core points are located inside a cluster</li>
<li>Border points are on the borders between two clusters</li>
<li>Neighborhood of p are all points within some radius of p, Eps</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-100" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Terminology</h1>

<p><space></p>

<ul>
<li>Core points are located inside a cluster</li>
<li>Border points are on the borders between two clusters</li>
<li>Neighborhood of p are all points within some radius of p, Eps
<img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/density_structure.png" alt="density"></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-101" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Terminology</h1>

<p><space></p>

<ul>
<li>Core points are located inside a cluster</li>
<li>Border points are on the borders between two clusters</li>
<li>Neighborhood of p are all points within some radius of p, Eps</li>
<li>High density region has at least Minpts within Eps of point p</li>
<li>Noise points are not within Eps of border or core points</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-102" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Terminology</h1>

<p><space></p>

<ul>
<li>Core points are located inside a cluster</li>
<li>Border points are on the borders between two clusters</li>
<li>Neighborhood of p are all points within some radius of p, Eps</li>
<li>High density region has at least Minpts within Eps of point p</li>
<li>Noise points are not within Eps of border or core points</li>
<li>If p is density connected to q, they are part of the same cluster, if not, then they are not</li>
<li>If p is not density connected to any other point, its considered noise</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-103" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>DBSCAN</h1>

<p><space></p>

<p><img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/density_ex_win.png" alt="density_win"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-104" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>DBSCAN</h1>

<p><space></p>

<pre><code class="r">x &lt;- c(2,2,8,5,7,6,1,4)
y &lt;- c(10,5,4,8,5,4,2,9)
cluster &lt;- data.frame(X=c(x,2*x,3*x),Y=c(y,-2*x,1/4*y))
plot(cluster)
</code></pre>

<p><img src="figure/unnamed-chunk-4.png" alt="plot of chunk unnamed-chunk-4"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-105" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>DBSCAN</h1>

<p><space></p>

<pre><code class="r">library(fpc)
cluster_DBSCAN&lt;-dbscan(cluster, eps=3, MinPts=2, method=&quot;hybrid&quot;)
plot(cluster_DBSCAN, cluster, main=&quot;Clustering using DBSCAN algorithm (eps=3, MinPts=3)&quot;)
</code></pre>

<p><img src="figure/dbscan_ex.png" alt="plot of chunk dbscan_ex"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-106" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article>
    <h1>Summary</h1>

<p><space></p>

<ul>
<li>Unsupervised learning</li>
<li>Not a perfect science - lots of interpretation

<ul>
<li>Dependent on values of K, Eps</li>
</ul></li>
<li>Hard to define &quot;correct&quot; clustering</li>
<li>Many types of algorithms</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-107" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Motivation</h1>

<p><space></p>

<ul>
<li>representation of decisions made in order to classify or predict
<img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/tree_example.png" alt="overview"></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-108" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Structure</h1>

<p><space></p>

<p><img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/tree_structure.png" alt="structure"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-109" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Structure</h1>

<p><space></p>

<ul>
<li>recursive partitioning -&gt; &quot;divide and conquer&quot;</li>
<li>going down, choose feature that is most <em>predictive</em> of target class

<ul>
<li>split the data according to feature</li>
<li>continue...</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-110" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Structure</h1>

<p><space></p>

<p>until...</p>

<ul>
<li>all examples at a node are in same class</li>
<li>no more features left to distinguish (prone to overfitting)</li>
<li>tree has grown to some prespecified limit (prune)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-111" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Algorithms</h1>

<p><space></p>

<ul>
<li>ID3

<ul>
<li>original, popular, DT implementation</li>
</ul></li>
<li>C4.5

<ul>
<li>like ID3 +</li>
<li>handles continuous cases</li>
<li>imputing missing values</li>
<li>weighing costs</li>
<li>pruning post creation</li>
</ul></li>
<li>C5.0

<ul>
<li>like C4.5 + </li>
<li>faster, less memory usage</li>
<li>boosting</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-112" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Selecting features</h1>

<p><space></p>

<ul>
<li>How does tree decide how to select feature?

<ul>
<li>purity of resulting split</li>
</ul></li>
<li><strong>Entropy</strong>: amount of information contained in a random variable

<ul>
<li>For a feature with N classes:</li>
<li>0 = purely homogenous</li>
<li>\(\log_{2}(N)\) = completely mixed</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-113" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy</h1>

<p><space></p>

<p>\(Entropy(S) = \sum_{i=1}^{c} -p_{i}\log_{2}(p_{i})\)</p>

<ul>
<li>where \(S\) is a dataset</li>
<li>\(c\) is the number of levels in that data</li>
<li>\(p_{i}\) is the proportion of values in that level</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-114" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy - example</h1>

<p><space></p>

<p>What is the entropy of a fair, 6 sided die?</p>

<pre><code class="r">entropy &lt;- function(probs){
  ent &lt;- 0
  for(i in probs){
    ent_temp &lt;- -i*log2(i)
    ent &lt;- ent + ent_temp
  }
  return(ent)
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-115" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy - example</h1>

<p><space></p>

<pre><code class="r">fair &lt;- rep(1/6,6)
entropy(fair)
</code></pre>

<pre><code>## [1] 2.585
</code></pre>

<pre><code class="r">log2(6)
</code></pre>

<pre><code>## [1] 2.585
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-116" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy - example</h1>

<p><space></p>

<p>What is the entropy of a biased, 6 sided die?</p>

<ul>
<li>\(P(X=1) = P(X=2) = P(X=3) = 1/9\)</li>
<li>\(P(X=4) = P(X=5) = P(X=6) = 2/9\)</li>
</ul>

<pre><code class="r">biased &lt;- c(rep(1/9,3),rep(2/9,3))
entropy(biased)
</code></pre>

<pre><code>[1] 2.503
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-117" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy - example</h1>

<p><space></p>

<pre><code class="r">more_biased &lt;- c(rep(1/18,3),rep(5/18,3))
entropy(more_biased)
</code></pre>

<pre><code>[1] 2.235
</code></pre>

<pre><code class="r">most_biased &lt;- c(rep(1/100,5),rep(95/100,1))
entropy(most_biased)
</code></pre>

<pre><code>[1] 0.4025
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-118" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy - example</h1>

<p><space></p>

<pre><code class="r">curve(-x*log2(x)-(1 - x)*log2(1 - x), col =&quot; red&quot;, xlab = &quot;x&quot;, ylab = &quot;Entropy&quot;, 
      lwd = 4, main=&#39;Entropy of a coin toss&#39;)
</code></pre>

<p><img src="figure/entropy_curve.png" alt="plot of chunk entropy_curve"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-119" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Entropy</h1>

<p><space></p>

<ul>
<li>C5.0 uses the change in entropy to determine the change in purity</li>
<li><p>InfoGain = Entropy (pre split) - Entropy (post split)</p>

<ul>
<li>Entropy (pre split) = current Entropy</li>
<li>Entropy (post split) is trickier</li>
<li>need to consider Entropy of each possible split</li>
<li>\(E(post) = \sum_{i=1}^{n}w_{i}Entropy(P_{i})\)</li>
</ul></li>
<li><p>Notes:</p>

<ul>
<li>The more a feature splits the data in obvious ways, the less informative it is, entropy is lower</li>
<li>The more a feature splits the data - in general - the higher the entropy and hence information gained by splitting at that feature</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-120" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">voting_data &lt;- read.csv(&#39;http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data&#39;)
names(voting_data) &lt;- c(&#39;party&#39;,&#39;handicapped-infants&#39;,&#39;water-project-cost-sharing&#39;,
                        &#39;adoption-of-the-budget-resolution&#39;,&#39;physician-fee-freeze&#39;,
                        &#39;el-salvador-aid&#39;,&#39;religious-groups-in-schools&#39;,
                        &#39;anti-satellite-test-ban&#39;,&#39;aid-to-nicaraguan-contras&#39;,
                        &#39;mx-missile&#39;,&#39;immigration&#39;,&#39;synfuels-corporation-cutback&#39;,
                        &#39;education-spending&#39;,&#39;superfund-right-to-sue&#39;,&#39;crime&#39;,
                        &#39;duty-free-exports&#39;,&#39;export-administration-act-south-africa&#39;)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-121" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r">prop.table(table(voting_data[,1]))
</code></pre>

<pre><code>
  democrat republican 
    0.6152     0.3848 
</code></pre>

<pre><code class="r">n &lt;- nrow(voting_data)
train_ind &lt;- sample(n,2/3*n)
voting_train &lt;- voting_data[train_ind,]
voting_test &lt;- voting_data[-train_ind,]
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-122" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<p><img src="/Users/ilanman/Desktop/Data/RPres_ML_2/figure/real_tree_example.png" height="500px" width="500px" /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-123" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code>

   Cell Contents
|-------------------------|
|                       N |
|         N / Table Total |
|-------------------------|


Total Observations in Table:  145 


             | predicted class 
actual class |   democrat | republican |  Row Total | 
-------------|------------|------------|------------|
    democrat |         98 |          1 |         99 | 
             |      0.676 |      0.007 |            | 
-------------|------------|------------|------------|
  republican |          2 |         44 |         46 | 
             |      0.014 |      0.303 |            | 
-------------|------------|------------|------------|
Column Total |        100 |         45 |        145 | 
-------------|------------|------------|------------|


</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-124" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r"># most important variables
head(C5imp(tree_model))
</code></pre>

<pre><code>##                                   Overall
## physician-fee-freeze                96.89
## synfuels-corporation-cutback        44.29
## mx-missile                          11.76
## adoption-of-the-budget-resolution    9.69
## handicapped-infants                  0.00
## water-project-cost-sharing           0.00
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-125" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example</h1>

<p><space></p>

<pre><code class="r"># in-sample error rate
summary(tree_model)
</code></pre>

<pre><code>## 
## Call:
## C5.0.default(x = voting_train[, -1], y = voting_train[, 1], trials = 1)
## 
## 
## C5.0 [Release 2.07 GPL Edition]      Tue Aug 26 09:00:38 2014
## -------------------------------
## 
## Class specified by attribute `outcome&#39;
## 
## Read 289 cases (17 attributes) from undefined.data
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (157.9/2.6)
## physician-fee-freeze = y:
## :...synfuels-corporation-cutback in {?,n}: republican (104.1/2.7)
##     synfuels-corporation-cutback = y:
##     :...mx-missile = ?: republican (0)
##         mx-missile = y: democrat (2.7)
##         mx-missile = n:
##         :...adoption-of-the-budget-resolution in {?,n}: republican (18.3/3.3)
##             adoption-of-the-budget-resolution = y: democrat (6.1/2)
## 
## 
## Evaluation on training data (289 cases):
## 
##      Decision Tree   
##    ----------------  
##    Size      Errors  
## 
##       5   11( 3.8%)   &lt;&lt;
## 
## 
##     (a)   (b)    &lt;-classified as
##    ----  ----
##     163     5    (a): class democrat
##       6   115    (b): class republican
## 
## 
##  Attribute usage:
## 
##   96.89% physician-fee-freeze
##   44.29% synfuels-corporation-cutback
##   11.76% mx-missile
##    9.69% adoption-of-the-budget-resolution
## 
## 
## Time: 0.0 secs
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-126" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Boosting</h1>

<p><space></p>

<ul>
<li>by combining a number of weak performing learners create a team that is much stronger than any one of the learners alone.</li>
<li>this is where C5.0 improves on C4.5</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-127" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example - Boosting</h1>

<p><space></p>

<pre><code class="r">boosted_tree_model &lt;- C5.0(voting_train[,-1],voting_train[,1], trials=25)
boosted_tennis_predict &lt;- predict(boosted_tree_model,voting_test[,-1])

boosted_conf &lt;- CrossTable(voting_test[,1], boosted_tennis_predict, prop.chisq = FALSE,
                           prop.c = FALSE, prop.r = FALSE, 
                           dnn = c(&quot;actual class&quot;, &quot;predicted class&quot;))
</code></pre>

<pre><code>## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |         N / Table Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  145 
## 
##  
##              | predicted class 
## actual class |   democrat | republican |  Row Total | 
## -------------|------------|------------|------------|
##     democrat |         98 |          1 |         99 | 
##              |      0.676 |      0.007 |            | 
## -------------|------------|------------|------------|
##   republican |          3 |         43 |         46 | 
##              |      0.021 |      0.297 |            | 
## -------------|------------|------------|------------|
## Column Total |        101 |         44 |        145 | 
## -------------|------------|------------|------------|
## 
## 
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-128" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Example - Boosting</h1>

<p><space></p>

<pre><code class="r"># in-sample error rate
summary(boosted_tree_model)
</code></pre>

<pre><code>## 
## Call:
## C5.0.default(x = voting_train[, -1], y = voting_train[, 1], trials = 25)
## 
## 
## C5.0 [Release 2.07 GPL Edition]      Tue Aug 26 09:00:38 2014
## -------------------------------
## 
## Class specified by attribute `outcome&#39;
## 
## Read 289 cases (17 attributes) from undefined.data
## 
## -----  Trial 0:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (157.9/2.6)
## physician-fee-freeze = y:
## :...synfuels-corporation-cutback in {?,n}: republican (104.1/2.7)
##     synfuels-corporation-cutback = y:
##     :...mx-missile = ?: republican (0)
##         mx-missile = y: democrat (2.7)
##         mx-missile = n:
##         :...adoption-of-the-budget-resolution in {?,n}: republican (18.3/3.3)
##             adoption-of-the-budget-resolution = y: democrat (6.1/2)
## 
## -----  Trial 1:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (134.6/16.9)
## physician-fee-freeze = y:
## :...immigration in {?,y}: republican (70.2/2.1)
##     immigration = n:
##     :...synfuels-corporation-cutback in {?,n}: republican (52.4/14.3)
##         synfuels-corporation-cutback = y: democrat (31.9/6.7)
## 
## -----  Trial 2:  -----
## 
## Decision tree:
## 
## education-spending in {?,n}: democrat (127.3/25.7)
## education-spending = y:
## :...adoption-of-the-budget-resolution in {?,n}: republican (106/13.4)
##     adoption-of-the-budget-resolution = y: democrat (55.7/23.9)
## 
## -----  Trial 3:  -----
## 
## Decision tree:
## 
## physician-fee-freeze = n: democrat (101/19.4)
## physician-fee-freeze in {?,y}: republican (188/46.9)
## 
## -----  Trial 4:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (90.2/18.9)
## physician-fee-freeze = y:
## :...water-project-cost-sharing in {?,n}: republican (69.4/11.7)
##     water-project-cost-sharing = y:
##     :...superfund-right-to-sue in {?,n}: democrat (18.6/1.6)
##         superfund-right-to-sue = y:
##         :...adoption-of-the-budget-resolution in {?,n}: republican (62/19.8)
##             adoption-of-the-budget-resolution = y: democrat (48.8/13.9)
## 
## -----  Trial 5:  -----
## 
## Decision tree:
## 
## synfuels-corporation-cutback = ?: republican (0)
## synfuels-corporation-cutback = n:
## :...education-spending in {?,y}: republican (84.1/12.7)
## :   education-spending = n:
## :   :...crime in {?,n}: democrat (20.4/2)
## :       crime = y: republican (58.7/22.3)
## synfuels-corporation-cutback = y:
## :...mx-missile in {?,y}: democrat (33.3/2.8)
##     mx-missile = n:
##     :...anti-satellite-test-ban in {?,n}: democrat (75.1/21.4)
##         anti-satellite-test-ban = y: republican (17.5/4.3)
## 
## -----  Trial 6:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (103.2/20.7)
## physician-fee-freeze = y:
## :...immigration in {?,y}: republican (68.7/10.8)
##     immigration = n:
##     :...anti-satellite-test-ban in {?,n}: democrat (96/38.4)
##         anti-satellite-test-ban = y: republican (21.1/3.5)
## 
## -----  Trial 7:  -----
## 
## Decision tree:
## 
## physician-fee-freeze = ?: republican (0)
## physician-fee-freeze = n: democrat (90.9/20.9)
## physician-fee-freeze = y:
## :...synfuels-corporation-cutback in {?,n}: republican (112/23.2)
##     synfuels-corporation-cutback = y:
##     :...mx-missile = n: republican (74.2/32.3)
##         mx-missile in {?,y}: democrat (11.9/0.9)
## 
## -----  Trial 8:  -----
## 
## Decision tree:
## 
## synfuels-corporation-cutback in {?,y}: democrat (133.6/37.8)
## synfuels-corporation-cutback = n:
## :...el-salvador-aid = n: democrat (38.8/12.2)
##     el-salvador-aid in {?,y}: republican (116.6/35.3)
## 
## -----  Trial 9:  -----
## 
## Decision tree:
## 
## adoption-of-the-budget-resolution in {?,y}: democrat (132.7/40.8)
## adoption-of-the-budget-resolution = n:
## :...education-spending = n: democrat (55.2/20.1)
##     education-spending in {?,y}: republican (101/22.4)
## 
## -----  Trial 10:  -----
## 
## Decision tree:
## 
## physician-fee-freeze = ?: republican (0)
## physician-fee-freeze = n: democrat (69.2/18.4)
## physician-fee-freeze = y:
## :...anti-satellite-test-ban in {?,y}: republican (56.2/5.1)
##     anti-satellite-test-ban = n:
##     :...immigration in {?,y}: republican (40.8/9.6)
##         immigration = n:
##         :...adoption-of-the-budget-resolution in {?,y}: democrat (25.7/2.9)
##             adoption-of-the-budget-resolution = n:
##             :...duty-free-exports = ?: republican (0)
##                 duty-free-exports = y: democrat (17.9/2.9)
##                 duty-free-exports = n:
##                 :...synfuels-corporation-cutback in {?,
##                     :                                n}: republican (28.3/0.1)
##                     synfuels-corporation-cutback = y: democrat (50.8/19.6)
## 
## -----  Trial 11:  -----
## 
## Decision tree:
## 
## el-salvador-aid = ?: republican (0)
## el-salvador-aid = n: democrat (62.2/17.3)
## el-salvador-aid = y:
## :...anti-satellite-test-ban in {?,y}: republican (55.8/6.1)
##     anti-satellite-test-ban = n:
##     :...adoption-of-the-budget-resolution = ?: republican (0)
##         adoption-of-the-budget-resolution = y: democrat (49/8)
##         adoption-of-the-budget-resolution = n:
##         :...superfund-right-to-sue = n: democrat (13.4/2.8)
##             superfund-right-to-sue in {?,y}: republican (108.5/30.8)
## 
## -----  Trial 12:  -----
## 
## Decision tree:
## 
## physician-fee-freeze = ?: republican (0)
## physician-fee-freeze = n: democrat (72.3/17.8)
## physician-fee-freeze = y:
## :...water-project-cost-sharing in {?,n}: republican (81.9/14.4)
##     water-project-cost-sharing = y:
##     :...superfund-right-to-sue in {?,n}: democrat (14.4/1)
##         superfund-right-to-sue = y:
##         :...duty-free-exports = ?: republican (0)
##             duty-free-exports = y: democrat (27.1/5.4)
##             duty-free-exports = n:
##             :...adoption-of-the-budget-resolution in {?,
##                 :                                     n}: republican (54.5/10.8)
##                 adoption-of-the-budget-resolution = y: democrat (38.8/14.9)
## 
## -----  Trial 13:  -----
## 
## Decision tree:
## 
## synfuels-corporation-cutback in {?,n}: republican (155.8/45.6)
## synfuels-corporation-cutback = y: democrat (133.2/42.3)
## 
## -----  Trial 14:  -----
## 
## Decision tree:
## 
## physician-fee-freeze = ?: democrat (0)
## physician-fee-freeze = n:
## :...adoption-of-the-budget-resolution = n: republican (25.6/10.1)
## :   adoption-of-the-budget-resolution in {?,y}: democrat (59.6/2.9)
## physician-fee-freeze = y:
## :...synfuels-corporation-cutback in {?,n}: republican (102.5/24.2)
##     synfuels-corporation-cutback = y: democrat (101.4/44.7)
## 
## -----  Trial 15:  -----
## 
## Decision tree:
## 
## physician-fee-freeze = ?: republican (0)
## physician-fee-freeze = n: democrat (86.1/16.6)
## physician-fee-freeze = y:
## :...immigration in {?,y}: republican (79.3/16)
##     immigration = n:
##     :...education-spending = n: democrat (40.3/11.6)
##         education-spending in {?,y}: republican (83.3/28.2)
## 
## -----  Trial 16:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (77/17.9)
## physician-fee-freeze = y:
## :...synfuels-corporation-cutback in {?,n}: republican (96.3/27.1)
##     synfuels-corporation-cutback = y:
##     :...mx-missile in {?,y}: democrat (18.5/0.6)
##         mx-missile = n:
##         :...anti-satellite-test-ban in {?,y}: republican (8.3)
##             anti-satellite-test-ban = n:
##             :...adoption-of-the-budget-resolution = n: republican (71.2/27.9)
##                 adoption-of-the-budget-resolution in {?,y}: democrat (17.6/0.6)
## 
## -----  Trial 17:  -----
## 
## Decision tree:
## 
## el-salvador-aid in {?,n}: democrat (51.8/12.2)
## el-salvador-aid = y:
## :...anti-satellite-test-ban in {?,y}: republican (63.2/11)
##     anti-satellite-test-ban = n:
##     :...adoption-of-the-budget-resolution in {?,y}: democrat (46.8/8.9)
##         adoption-of-the-budget-resolution = n:
##         :...immigration = ?: democrat (0)
##             immigration = y: republican (22.6/4.2)
##             immigration = n:
##             :...superfund-right-to-sue in {?,n}: democrat (16.9/1.8)
##                 superfund-right-to-sue = y:
##                 :...mx-missile in {?,y}: democrat (2.9/0.2)
##                     mx-missile = n:
##                     :...education-spending in {?,n}: democrat (39.7/13.1)
##                         education-spending = y: republican (45.1/17.8)
## 
## -----  Trial 18:  -----
## 
## Decision tree:
## 
## synfuels-corporation-cutback = ?: democrat (0)
## synfuels-corporation-cutback = n:
## :...crime = n: democrat (19.4/5.1)
## :   crime in {?,y}: republican (130.9/42.8)
## synfuels-corporation-cutback = y:
## :...physician-fee-freeze in {?,n}: democrat (31.4/2.3)
##     physician-fee-freeze = y:
##     :...mx-missile in {?,y}: democrat (13.1/1)
##         mx-missile = n:
##         :...export-administration-act-south-africa in {?,
##             :                                          n}: democrat (56.6/16.4)
##             export-administration-act-south-africa = y: republican (37.6/10.6)
## 
## -----  Trial 19:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (92.9/21.4)
## physician-fee-freeze = y:
## :...immigration in {?,y}: republican (60.2/13)
##     immigration = n:
##     :...anti-satellite-test-ban = ?: democrat (0)
##         anti-satellite-test-ban = y: republican (16.2/2.5)
##         anti-satellite-test-ban = n:
##         :...adoption-of-the-budget-resolution in {?,y}: democrat (24.3/2.4)
##             adoption-of-the-budget-resolution = n:
##             :...religious-groups-in-schools = ?: democrat (0)
##                 religious-groups-in-schools = n: republican (4.3/0.1)
##                 religious-groups-in-schools = y:
##                 :...superfund-right-to-sue in {?,n}: democrat (10/0.7)
##                     superfund-right-to-sue = y:
##                     :...duty-free-exports in {?,n}: republican (64.8/26.7)
##                         duty-free-exports = y: democrat (16.4/4.1)
## 
## -----  Trial 20:  -----
## 
## Decision tree:
## 
## handicapped-infants in {?,y}: democrat (90.4/24)
## handicapped-infants = n:
## :...synfuels-corporation-cutback in {?,n}: republican (107.3/37.6)
##     synfuels-corporation-cutback = y: democrat (91.4/37.3)
## 
## -----  Trial 21:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (89/23.4)
## physician-fee-freeze = y:
## :...immigration in {?,y}: republican (66.8/17.5)
##     immigration = n:
##     :...synfuels-corporation-cutback = n: republican (61.1/24.6)
##         synfuels-corporation-cutback in {?,y}: democrat (72.1/27.4)
## 
## -----  Trial 22:  -----
## 
## Decision tree:
## 
## adoption-of-the-budget-resolution in {?,y}: democrat (129.2/36.1)
## adoption-of-the-budget-resolution = n:
## :...synfuels-corporation-cutback in {?,n}: republican (54.9/4.9)
##     synfuels-corporation-cutback = y:
##     :...mx-missile in {?,n}: republican (92.6/36.7)
##         mx-missile = y: democrat (10.3/0.5)
## 
## -----  Trial 23:  -----
## 
## Decision tree:
## 
## physician-fee-freeze = ?: republican (0)
## physician-fee-freeze = n: democrat (94.7/23.3)
## physician-fee-freeze = y:
## :...synfuels-corporation-cutback in {?,n}: republican (81.9/3.8)
##     synfuels-corporation-cutback = y:
##     :...mx-missile in {?,n}: republican (90.7/31.9)
##         mx-missile = y: democrat (17.7/0.9)
## 
## -----  Trial 24:  -----
## 
## Decision tree:
## 
## physician-fee-freeze in {?,n}: democrat (67.6/1.7)
## physician-fee-freeze = y:
## :...synfuels-corporation-cutback in {?,n}: republican (67.4/2.7)
##     synfuels-corporation-cutback = y:
##     :...water-project-cost-sharing = n: republican (14.5/0.6)
##         water-project-cost-sharing in {?,y}: democrat (133.5/33.9)
## 
## 
## Evaluation on training data (289 cases):
## 
## Trial        Decision Tree   
## -----      ----------------  
##    Size      Errors  
## 
##    0      5   11( 3.8%)
##    1      4   15( 5.2%)
##    2      3   32(11.1%)
##    3      2   17( 5.9%)
##    4      5   13( 4.5%)
##    5      6   52(18.0%)
##    6      4   48(16.6%)
##    7      4    9( 3.1%)
##    8      3   35(12.1%)
##    9      3   32(11.1%)
##   10      7   13( 4.5%)
##   11      5   32(11.1%)
##   12      6   12( 4.2%)
##   13      2  101(34.9%)
##   14      4   31(10.7%)
##   15      4   15( 5.2%)
##   16      6    8( 2.8%)
##   17      8   31(10.7%)
##   18      6   35(12.1%)
##   19      8   12( 4.2%)
##   20      3   57(19.7%)
##   21      4   17( 5.9%)
##   22      4   32(11.1%)
##   23      4   12( 4.2%)
##   24      4   19( 6.6%)
## boost              6( 2.1%)   &lt;&lt;
## 
## 
##     (a)   (b)    &lt;-classified as
##    ----  ----
##     165     3    (a): class democrat
##       3   118    (b): class republican
## 
## 
##  Attribute usage:
## 
##   97.23% adoption-of-the-budget-resolution
##   96.89% physician-fee-freeze
##   96.54% handicapped-infants
##   96.54% el-salvador-aid
##   95.50% synfuels-corporation-cutback
##   92.04% education-spending
##   61.94% crime
##   58.82% anti-satellite-test-ban
##   47.06% immigration
##   46.71% mx-missile
##   40.83% water-project-cost-sharing
##   37.72% superfund-right-to-sue
##   29.76% duty-free-exports
##   15.92% religious-groups-in-schools
##    8.30% export-administration-act-south-africa
## 
## 
## Time: 0.0 secs
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-129" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Error Cost</h1>

<p><space></p>

<ul>
<li>still getting too many false positives (predict republican but actually democrat)</li>
<li>introduce higher cost to getting this wrong</li>
</ul>

<pre><code class="r">error_cost &lt;- matrix(c(0,1,2,0),nrow=2)
cost_model &lt;- C5.0(voting_train[,-1],voting_train[,1], trials=1, costs = error_cost)
</code></pre>

<pre><code>## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
</code></pre>

<pre><code class="r">cost_predict &lt;- predict(cost_model, newdata=voting_test[,-1])
conf &lt;- CrossTable(voting_test[,1], cost_predict, prop.chisq = FALSE,
                   prop.c = FALSE, prop.r = FALSE,
                   dnn = c(&quot;actual class&quot;, &quot;predicted class&quot;))
</code></pre>

<pre><code>## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |         N / Table Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  145 
## 
##  
##              | predicted class 
## actual class |   democrat | republican |  Row Total | 
## -------------|------------|------------|------------|
##     democrat |         95 |          4 |         99 | 
##              |      0.655 |      0.028 |            | 
## -------------|------------|------------|------------|
##   republican |          2 |         44 |         46 | 
##              |      0.014 |      0.303 |            | 
## -------------|------------|------------|------------|
## Column Total |         97 |         48 |        145 | 
## -------------|------------|------------|------------|
## 
## 
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-130" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Error Cost</h1>

<p><space></p>

<pre><code>## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
## 
## Warning: 
## no dimnames were given for the cost matrix; the factor levels will be used
</code></pre>

<p><img src="figure/plot_boost_acc.png" alt="plot of chunk plot_boost_acc"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-131" style="background:;">
  <hgroup>
    <h2>Trees</h2>
  </hgroup>
  <article>
    <h1>Pros and Cons</h1>

<p><space></p>

<ul>
<li>trees are non-parametric, rule based classification or regression method</li>
<li>simple to understand and interpret</li>
<li>little data preparation</li>
<li>works well with small or large number of features
<br></li>
<li>easy to overfit</li>
<li>biased towards splits on features with large number of levels</li>
<li>usually finds local optimum</li>
<li>difficult concepts are hard to learn</li>
<li>avoid pre-pruning</li>
<li>hard to know optimal length of tree without growing it there first</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-132" style="background:;">
  <hgroup>
    <h2>Summary</h2>
  </hgroup>
  <article>
    <h1>ML - Part II</h1>

<p><space></p>

<ul>
<li>Logistic regression</li>
<li>Math behind PCA</li>
<li>3 types of clusters</li>
<li>Trees and improvements</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-133" style="background:;">
  <hgroup>
    <h2>Resources</h2>
  </hgroup>
  <article>
    <p><space></p>

<ul>
<li><a href="http://www.packtpub.com/machine-learning-with-r/book">Machine Learning with R</a></li>
<li><a href="http://shop.oreilly.com/product/0636920018483.do">Machine Learning for Hackers</a></li>
<li><a href="http://web.stanford.edu/%7Ehastie/local.ftp/Springer/OLD/ESLII_print4.pdf">Elements of Statistical Learning</a></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-134" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    
  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>

  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
<!-- Grab CDN jQuery, fall back to local if offline -->
<script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
<script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery-1.7.min.js"><\/script>')</script>
<!-- Load Javascripts for Widgets -->
<!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->
<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
<script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- DONE LOADING HIGHLIGHTER JS FILES -->
</html>
